{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于滑动窗口策略的机器阅读理解任务实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是阅读理解的答案在文档本身里，需要定位答案在文档中的起始位置和结束位置。如果文档超过max length就截断的话问题很大，所以采用滑动窗口的思想，将其分为多个部分重叠的文档，然后在每个文档上定位答案的起始位置和结束位置，最终选择所有文档中分数最高的作为原文档中答案的起始位置和结束位置\n",
    "\n",
    "评价指标包括精准匹配度（EM），表示预测结果与标准答案是否完全匹配，以及模糊匹配度（F1），通过字级别的P和R统计匹配程度\n",
    "\n",
    "标签包括起始位置和具体的答案，需要进一步处理成起始token的位置和结束token的位置！有时是以列表的形式呈现，因为答案可能出现在不止一个地方或不止一个答案\n",
    "\n",
    "最终的输出形状是[batch_size, seq_length, 2]，拆成起始位置的logits：[batch_size, seq_length]和结束位置的logits：[batch_size, seq_length]，softmax是在seq_length这个维度上进行计算的，loss是起始和结束位置的CEloss平均值！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果可以联网，直接使用load_dataset进行加载\n",
    "#datasets = load_dataset(\"cmrc2018\", cache_dir=\"data\")\n",
    "# 如果无法联网，则使用下面的方式加载数据集\n",
    "datasets = DatasetDict.load_from_disk(\"mrc_data\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TRAIN_186_QUERY_0',\n",
       " 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       " 'question': '范廷颂是什么时候被任为主教的？',\n",
       " 'answers': {'text': ['1963年'], 'answer_start': [30]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='/data/PLM/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/PLM/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = datasets[\"train\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples = tokenizer(text=sample_dataset[\"question\"],\n",
    "                               text_pair=sample_dataset[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               # 滑动窗口的构造通过下面两句话实现\n",
    "                               return_overflowing_tokens=True, \n",
    "                               stride=128, # 两个滑动窗口之间重叠128个tokens，理解成128个token ids也行，因为是一一对应的\n",
    "                               max_length=384, truncation=\"only_second\", padding=\"max_length\") # 填充肯定要做，截断可能是为了不让训练数据过大\n",
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9]\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_examples[\"overflow_to_sample_mapping\"]) # 表示属于第几个原文档\n",
    "print(len(tokenized_examples[\"overflow_to_sample_mapping\"])) # 这里就是把10个原文档用滑动窗口分成了29个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 [SEP]\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 [SEP]\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 1990 年 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 [SEP]\n",
      "[CLS] 1990 年 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， [SEP]\n",
      "[CLS] 1990 年 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 范 廷 颂 是 于 何 时 何 地 出 生 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 [SEP]\n",
      "[CLS] 范 廷 颂 是 于 何 时 何 地 出 生 的 ？ [SEP] 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， [SEP]\n",
      "[CLS] 范 廷 颂 是 于 何 时 何 地 出 生 的 ？ [SEP] 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 1994 年 3 月 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 [SEP]\n",
      "[CLS] 1994 年 3 月 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 [SEP]\n",
      "[CLS] 1994 年 3 月 ， 范 廷 颂 担 任 什 么 职 务 ？ [SEP] 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 范 廷 颂 是 何 时 去 世 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 [SEP]\n",
      "[CLS] 范 廷 颂 是 何 时 去 世 的 ？ [SEP] 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 [SEP]\n",
      "[CLS] 范 廷 颂 是 何 时 去 世 的 ？ [SEP] 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 安 雅 · 罗 素 法 参 加 了 什 么 比 赛 获 得 了 亚 军 ？ [SEP] 安 雅 · 罗 素 法 （ ， ） ， 来 自 俄 罗 斯 圣 彼 得 堡 的 模 特 儿 。 她 是 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季 的 亚 军 。 2008 年 ， 安 雅 宣 布 改 回 出 生 时 的 名 字 ： 安 雅 · 罗 素 法 （ anya rozova ） ， 在 此 之 前 是 使 用 安 雅 · 冈 （ ） 。 安 雅 于 俄 罗 斯 出 生 ， 后 来 被 一 个 居 住 在 美 国 夏 威 夷 群 岛 欧 胡 岛 檀 香 山 的 家 庭 领 养 。 安 雅 十 七 岁 时 曾 参 与 香 奈 儿 、 路 易 · 威 登 及 芬 迪 （ fendi ） 等 品 牌 的 非 正 式 时 装 秀 。 2007 年 ， 她 于 瓦 伊 帕 胡 高 级 中 学 毕 业 。 毕 业 后 ， 她 当 了 一 名 售 货 员 。 她 曾 为 russell tanoue 拍 摄 照 片 ， russell tanoue 称 赞 她 是 「 有 前 途 的 新 面 孔 」 。 安 雅 在 半 准 决 赛 面 试 时 说 她 对 模 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 [SEP]\n",
      "[CLS] 安 雅 · 罗 素 法 参 加 了 什 么 比 赛 获 得 了 亚 军 ？ [SEP] 模 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚 军 ( 但 就 整 体 表 现 来 说 ， 部 份 网 友 认 为 安 雅 才 是 第 十 季 名 副 其 实 的 冠 军 。 ) 安 雅 在 比 赛 拿 五 次 第 一 ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 [SEP]\n",
      "[CLS] 安 雅 · 罗 素 法 参 加 了 什 么 比 赛 获 得 了 亚 军 ？ [SEP] 第 一 ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 特 儿 ， 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] russell tanoue 对 安 雅 · 罗 素 法 的 评 价 是 什 么 ？ [SEP] 安 雅 · 罗 素 法 （ ， ） ， 来 自 俄 罗 斯 圣 彼 得 堡 的 模 特 儿 。 她 是 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季 的 亚 军 。 2008 年 ， 安 雅 宣 布 改 回 出 生 时 的 名 字 ： 安 雅 · 罗 素 法 （ anya rozova ） ， 在 此 之 前 是 使 用 安 雅 · 冈 （ ） 。 安 雅 于 俄 罗 斯 出 生 ， 后 来 被 一 个 居 住 在 美 国 夏 威 夷 群 岛 欧 胡 岛 檀 香 山 的 家 庭 领 养 。 安 雅 十 七 岁 时 曾 参 与 香 奈 儿 、 路 易 · 威 登 及 芬 迪 （ fendi ） 等 品 牌 的 非 正 式 时 装 秀 。 2007 年 ， 她 于 瓦 伊 帕 胡 高 级 中 学 毕 业 。 毕 业 后 ， 她 当 了 一 名 售 货 员 。 她 曾 为 russell tanoue 拍 摄 照 片 ， russell tanoue 称 赞 她 是 「 有 前 途 的 新 面 孔 」 。 安 雅 在 半 准 决 赛 面 试 时 说 她 对 模 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 [SEP]\n",
      "[CLS] russell tanoue 对 安 雅 · 罗 素 法 的 评 价 是 什 么 ？ [SEP] 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚 军 ( 但 就 整 体 表 现 来 说 ， 部 份 网 友 认 为 安 雅 才 是 第 十 季 名 副 其 实 的 冠 军 。 ) 安 雅 在 比 赛 拿 五 次 第 一 ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 [SEP]\n",
      "[CLS] russell tanoue 对 安 雅 · 罗 素 法 的 评 价 是 什 么 ？ [SEP] ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 特 儿 ， 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 安 雅 · 罗 素 法 合 作 过 的 香 港 杂 志 有 哪 些 ？ [SEP] 安 雅 · 罗 素 法 （ ， ） ， 来 自 俄 罗 斯 圣 彼 得 堡 的 模 特 儿 。 她 是 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季 的 亚 军 。 2008 年 ， 安 雅 宣 布 改 回 出 生 时 的 名 字 ： 安 雅 · 罗 素 法 （ anya rozova ） ， 在 此 之 前 是 使 用 安 雅 · 冈 （ ） 。 安 雅 于 俄 罗 斯 出 生 ， 后 来 被 一 个 居 住 在 美 国 夏 威 夷 群 岛 欧 胡 岛 檀 香 山 的 家 庭 领 养 。 安 雅 十 七 岁 时 曾 参 与 香 奈 儿 、 路 易 · 威 登 及 芬 迪 （ fendi ） 等 品 牌 的 非 正 式 时 装 秀 。 2007 年 ， 她 于 瓦 伊 帕 胡 高 级 中 学 毕 业 。 毕 业 后 ， 她 当 了 一 名 售 货 员 。 她 曾 为 russell tanoue 拍 摄 照 片 ， russell tanoue 称 赞 她 是 「 有 前 途 的 新 面 孔 」 。 安 雅 在 半 准 决 赛 面 试 时 说 她 对 模 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 [SEP]\n",
      "[CLS] 安 雅 · 罗 素 法 合 作 过 的 香 港 杂 志 有 哪 些 ？ [SEP] 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚 军 ( 但 就 整 体 表 现 来 说 ， 部 份 网 友 认 为 安 雅 才 是 第 十 季 名 副 其 实 的 冠 军 。 ) 安 雅 在 比 赛 拿 五 次 第 一 ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 [SEP]\n",
      "[CLS] 安 雅 · 罗 素 法 合 作 过 的 香 港 杂 志 有 哪 些 ？ [SEP] ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 特 儿 ， 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 毕 业 后 的 安 雅 · 罗 素 法 职 业 是 什 么 ？ [SEP] 安 雅 · 罗 素 法 （ ， ） ， 来 自 俄 罗 斯 圣 彼 得 堡 的 模 特 儿 。 她 是 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季 的 亚 军 。 2008 年 ， 安 雅 宣 布 改 回 出 生 时 的 名 字 ： 安 雅 · 罗 素 法 （ anya rozova ） ， 在 此 之 前 是 使 用 安 雅 · 冈 （ ） 。 安 雅 于 俄 罗 斯 出 生 ， 后 来 被 一 个 居 住 在 美 国 夏 威 夷 群 岛 欧 胡 岛 檀 香 山 的 家 庭 领 养 。 安 雅 十 七 岁 时 曾 参 与 香 奈 儿 、 路 易 · 威 登 及 芬 迪 （ fendi ） 等 品 牌 的 非 正 式 时 装 秀 。 2007 年 ， 她 于 瓦 伊 帕 胡 高 级 中 学 毕 业 。 毕 业 后 ， 她 当 了 一 名 售 货 员 。 她 曾 为 russell tanoue 拍 摄 照 片 ， russell tanoue 称 赞 她 是 「 有 前 途 的 新 面 孔 」 。 安 雅 在 半 准 决 赛 面 试 时 说 她 对 模 特 儿 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 [SEP]\n",
      "[CLS] 毕 业 后 的 安 雅 · 罗 素 法 职 业 是 什 么 ？ [SEP] 行 业 充 满 热 诚 ， 所 以 参 加 全 美 超 级 模 特 儿 新 秀 大 赛 。 她 于 比 赛 中 表 现 出 色 ， 曾 五 次 首 名 入 围 ， 平 均 入 围 顺 序 更 拿 下 历 届 以 来 最 优 异 的 成 绩 ( 2. 64 ) ， 另 外 胜 出 三 次 小 挑 战 ， 分 别 获 得 与 评 判 尼 祖 · 百 克 拍 照 、 为 柠 檬 味 道 的 七 喜 拍 摄 广 告 的 机 会 及 十 万 美 元 、 和 盖 马 蒂 洛 （ gai mattiolo ） 设 计 的 晚 装 。 在 最 后 两 强 中 ， 安 雅 与 另 一 名 参 赛 者 惠 妮 · 汤 姆 森 为 范 思 哲 走 秀 ， 但 评 判 认 为 她 在 台 上 不 够 惠 妮 突 出 ， 所 以 选 了 惠 妮 当 冠 军 ， 安 雅 屈 居 亚 军 ( 但 就 整 体 表 现 来 说 ， 部 份 网 友 认 为 安 雅 才 是 第 十 季 名 副 其 实 的 冠 军 。 ) 安 雅 在 比 赛 拿 五 次 第 一 ， 也 胜 出 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 [SEP]\n",
      "[CLS] 毕 业 后 的 安 雅 · 罗 素 法 职 业 是 什 么 ？ [SEP] 多 次 小 挑 战 。 安 雅 赛 后 再 次 与 russell tanoue 合 作 ， 为 2008 年 4 月 30 日 出 版 的 midweek 杂 志 拍 摄 封 面 及 内 页 照 。 其 后 她 参 加 了 v 杂 志 与 supreme 模 特 儿 公 司 合 办 的 模 特 儿 选 拔 赛 2008 。 她 其 后 更 与 elite 签 约 。 最 近 她 与 香 港 的 模 特 儿 公 司 style international management 签 约 ， 并 在 香 港 发 展 其 模 特 儿 事 业 。 她 曾 在 很 多 香 港 的 时 装 杂 志 中 任 模 特 儿 ， 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 岬 太 郎 在 第 一 次 南 葛 市 生 活 时 的 搭 档 是 谁 ？ [SEP] 为 日 本 漫 画 足 球 小 将 翼 的 一 个 角 色 ， 自 小 父 母 离 异 ， 与 父 亲 一 起 四 处 为 家 ， 每 个 地 方 也 是 待 一 会 便 离 开 ， 但 他 仍 然 能 够 保 持 优 秀 的 学 业 成 绩 。 在 第 一 次 南 葛 市 生 活 时 ， 与 同 样 就 读 于 南 葛 小 学 的 大 空 翼 为 黄 金 拍 档 ， 曾 效 力 球 队 包 括 南 葛 小 学 、 南 葛 高 中 、 日 本 少 年 队 、 日 本 青 年 军 、 日 本 奥 运 队 。 效 力 日 本 青 年 军 期 间 ， 因 救 同 母 异 父 的 妹 妹 导 致 被 车 撞 至 断 脚 ， 在 决 赛 周 只 在 决 赛 的 下 半 场 十 五 分 钟 开 始 上 场 ， 成 为 日 本 队 夺 得 世 青 冠 军 的 其 中 一 名 功 臣 。 基 本 资 料 绰 号 ： 球 场 上 的 艺 术 家 出 身 地 ： 日 本 南 葛 市 诞 生 日 ： 5 月 5 日 星 座 ： 金 牛 座 球 衣 号 码 ： 11 担 任 位 置 ： 中 场 、 攻 击 中 场 、 右 中 场 擅 长 脚 ： 右 脚 所 属 队 伍 ： 盘 田 山 叶 故 事 发 展 岬 太 郎 在 小 学 期 间 不 断 转 换 学 校 ， 在 南 葛 小 学 就 读 时 在 全 国 大 赛 中 夺 得 冠 军 ； 国 中 三 年 随 父 亲 孤 单 地 在 法 国 留 学 ； 回 国 后 三 年 的 高 中 生 涯 一 直 输 给 日 本 王 牌 射 手 日 向 小 次 郎 率 领 的 东 邦 学 院 。 在 【 golden 23 】 [SEP]\n",
      "[CLS] 岬 太 郎 在 第 一 次 南 葛 市 生 活 时 的 搭 档 是 谁 ？ [SEP] 衣 号 码 ： 11 担 任 位 置 ： 中 场 、 攻 击 中 场 、 右 中 场 擅 长 脚 ： 右 脚 所 属 队 伍 ： 盘 田 山 叶 故 事 发 展 岬 太 郎 在 小 学 期 间 不 断 转 换 学 校 ， 在 南 葛 小 学 就 读 时 在 全 国 大 赛 中 夺 得 冠 军 ； 国 中 三 年 随 父 亲 孤 单 地 在 法 国 留 学 ； 回 国 后 三 年 的 高 中 生 涯 一 直 输 给 日 本 王 牌 射 手 日 向 小 次 郎 率 领 的 东 邦 学 院 。 在 【 golden 23 】 年 代 ， 大 空 翼 、 日 向 小 次 郎 等 名 将 均 转 战 海 外 ， 他 与 松 山 光 、 三 杉 淳 组 成 了 「 3m 」 组 合 （ 松 山 光 hikaru matsuyama 、 岬 太 郎 taro misaki 、 三 杉 淳 jyun misugi ） 。 必 杀 技 1. 回 力 刀 射 门 2. s. s. s. 射 门 3. 双 人 射 门 ( 与 大 空 翼 合 作 ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for sen in tokenizer.batch_decode(tokenized_examples[\"input_ids\"]):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 105), (105, 106), (106, 107), (107, 108), (108, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 119), (119, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 126), (126, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 133), (133, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 140), (140, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 147), (147, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 154), (154, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 186), (186, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 192), (192, 193), (193, 194), (194, 195), (195, 196), (196, 197), (197, 198), (198, 199), (199, 200), (200, 201), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 207), (207, 208), (208, 209), (209, 210), (210, 211), (211, 212), (212, 213), (213, 214), (214, 215), (215, 216), (216, 217), (217, 218), (218, 222), (222, 223), (223, 224), (224, 225), (225, 226), (226, 227), (227, 228), (228, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 243), (243, 244), (244, 245), (245, 246), (246, 247), (247, 248), (248, 249), (249, 250), (250, 251), (251, 252), (252, 253), (253, 257), (257, 258), (258, 259), (259, 260), (260, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (332, 333), (333, 334), (334, 335), (335, 336), (336, 337), (337, 338), (338, 339), (339, 340), (340, 341), (341, 342), (342, 343), (343, 344), (344, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 360), (360, 361), (361, 362), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 368), (368, 369), (369, 370), (370, 371), (371, 372), (372, 373), (373, 374), (374, 375), (375, 376), (376, 377), (377, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 385), (385, 386), (386, 387), (387, 388), (388, 390), (390, 391), (391, 392), (392, 393), (393, 394), (394, 395), (395, 396), (396, 397), (397, 398), (398, 399), (399, 400), (400, 401), (0, 0)]\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (266, 267), (267, 268), (268, 269), (269, 270), (270, 271), (271, 272), (272, 273), (273, 274), (274, 275), (275, 276), (276, 277), (277, 278), (278, 279), (279, 280), (280, 281), (281, 282), (282, 283), (283, 284), (284, 285), (285, 286), (286, 287), (287, 288), (288, 289), (289, 290), (290, 291), (291, 292), (292, 293), (293, 294), (294, 295), (295, 296), (296, 297), (297, 298), (298, 299), (299, 300), (300, 301), (301, 302), (302, 303), (303, 304), (304, 305), (305, 306), (306, 307), (307, 308), (308, 309), (309, 310), (310, 311), (311, 312), (312, 313), (313, 314), (314, 315), (315, 316), (316, 317), (317, 318), (318, 319), (319, 320), (320, 321), (321, 325), (325, 326), (326, 327), (327, 328), (328, 329), (329, 330), (330, 331), (331, 332), (332, 333), (333, 334), (334, 335), (335, 336), (336, 337), (337, 338), (338, 339), (339, 340), (340, 341), (341, 342), (342, 343), (343, 344), (344, 345), (345, 346), (346, 347), (347, 348), (348, 349), (349, 350), (350, 351), (351, 352), (352, 353), (353, 354), (354, 355), (355, 356), (356, 360), (360, 361), (361, 362), (362, 363), (363, 364), (364, 365), (365, 366), (366, 367), (367, 368), (368, 369), (369, 370), (370, 371), (371, 372), (372, 373), (373, 374), (374, 375), (375, 376), (376, 377), (377, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 385), (385, 386), (386, 387), (387, 388), (388, 390), (390, 391), (391, 392), (392, 393), (393, 394), (394, 395), (395, 396), (396, 397), (397, 398), (398, 399), (399, 400), (400, 401), (401, 402), (402, 403), (403, 404), (404, 405), (405, 406), (406, 407), (407, 408), (408, 409), (409, 410), (410, 411), (411, 412), (412, 413), (413, 414), (414, 415), (415, 416), (416, 417), (417, 418), (418, 419), (419, 420), (420, 421), (421, 422), (422, 424), (424, 425), (425, 426), (426, 427), (427, 428), (428, 429), (429, 430), (430, 431), (431, 432), (432, 433), (433, 434), (434, 435), (435, 436), (436, 437), (437, 438), (438, 439), (439, 440), (440, 441), (441, 442), (442, 443), (443, 444), (444, 445), (445, 446), (446, 447), (447, 448), (448, 449), (449, 450), (450, 451), (451, 452), (452, 453), (453, 454), (454, 455), (455, 456), (456, 457), (457, 458), (458, 459), (459, 460), (460, 461), (461, 462), (462, 463), (463, 464), (464, 465), (465, 466), (466, 467), (467, 468), (468, 469), (469, 470), (470, 471), (471, 472), (472, 473), (473, 474), (474, 475), (475, 476), (476, 477), (477, 478), (478, 479), (479, 480), (480, 481), (481, 482), (482, 483), (483, 484), (484, 485), (485, 486), (486, 487), (487, 488), (488, 489), (489, 490), (490, 491), (491, 492), (492, 493), (493, 494), (494, 495), (495, 499), (499, 500), (500, 501), (501, 502), (502, 503), (503, 504), (504, 505), (505, 506), (506, 507), (507, 508), (508, 509), (509, 510), (510, 511), (511, 512), (512, 513), (513, 514), (514, 516), (516, 517), (517, 518), (518, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 532), (532, 533), (533, 534), (534, 535), (535, 536), (536, 537), (537, 538), (538, 539), (539, 540), (540, 541), (541, 542), (542, 543), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 552), (552, 553), (553, 554), (554, 555), (555, 557), (557, 558), (558, 559), (559, 560), (560, 561), (561, 562), (562, 563), (563, 564), (564, 565), (565, 566), (566, 567), (567, 568), (568, 569), (569, 570), (570, 571), (571, 572), (572, 573), (573, 574), (574, 575), (575, 576), (576, 577), (577, 578), (578, 579), (579, 580), (580, 581), (581, 582), (582, 583), (583, 584), (584, 585), (585, 586), (586, 587), (587, 588), (588, 589), (589, 590), (590, 591), (591, 592), (592, 593), (593, 594), (594, 595), (595, 596), (596, 597), (597, 598), (598, 599), (599, 600), (600, 601), (601, 603), (603, 604), (604, 606), (606, 607), (607, 608), (608, 609), (609, 610), (610, 611), (611, 612), (612, 613), (613, 614), (614, 615), (615, 616), (616, 617), (617, 618), (618, 619), (619, 620), (620, 621), (621, 622), (622, 623), (623, 624), (624, 625), (625, 626), (626, 627), (627, 631), (631, 632), (632, 633), (633, 637), (637, 638), (638, 639), (639, 640), (640, 641), (641, 642), (642, 643), (643, 644), (644, 645), (645, 646), (646, 647), (647, 648), (648, 649), (649, 650), (650, 651), (651, 652), (652, 653), (653, 657), (657, 658), (658, 659), (0, 0)]\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (516, 517), (517, 518), (518, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 532), (532, 533), (533, 534), (534, 535), (535, 536), (536, 537), (537, 538), (538, 539), (539, 540), (540, 541), (541, 542), (542, 543), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 552), (552, 553), (553, 554), (554, 555), (555, 557), (557, 558), (558, 559), (559, 560), (560, 561), (561, 562), (562, 563), (563, 564), (564, 565), (565, 566), (566, 567), (567, 568), (568, 569), (569, 570), (570, 571), (571, 572), (572, 573), (573, 574), (574, 575), (575, 576), (576, 577), (577, 578), (578, 579), (579, 580), (580, 581), (581, 582), (582, 583), (583, 584), (584, 585), (585, 586), (586, 587), (587, 588), (588, 589), (589, 590), (590, 591), (591, 592), (592, 593), (593, 594), (594, 595), (595, 596), (596, 597), (597, 598), (598, 599), (599, 600), (600, 601), (601, 603), (603, 604), (604, 606), (606, 607), (607, 608), (608, 609), (609, 610), (610, 611), (611, 612), (612, 613), (613, 614), (614, 615), (615, 616), (616, 617), (617, 618), (618, 619), (619, 620), (620, 621), (621, 622), (622, 623), (623, 624), (624, 625), (625, 626), (626, 627), (627, 631), (631, 632), (632, 633), (633, 637), (637, 638), (638, 639), (639, 640), (640, 641), (641, 642), (642, 643), (643, 644), (644, 645), (645, 646), (646, 647), (647, 648), (648, 649), (649, 650), (650, 651), (651, 652), (652, 653), (653, 657), (657, 658), (658, 659), (659, 660), (660, 662), (662, 663), (663, 664), (664, 665), (665, 666), (666, 667), (667, 668), (668, 669), (669, 670), (670, 671), (671, 672), (672, 673), (673, 674), (674, 675), (675, 676), (676, 677), (677, 678), (678, 679), (679, 680), (680, 681), (681, 682), (682, 683), (683, 684), (684, 685), (685, 686), (686, 687), (687, 688), (688, 689), (689, 690), (690, 691), (691, 692), (692, 693), (693, 694), (694, 695), (695, 696), (696, 697), (697, 698), (698, 699), (699, 700), (700, 701), (701, 702), (702, 703), (703, 704), (704, 705), (705, 706), (706, 707), (707, 708), (708, 709), (709, 710), (710, 714), (714, 715), (715, 716), (716, 717), (717, 719), (719, 720), (720, 721), (721, 722), (722, 723), (723, 724), (724, 725), (725, 726), (726, 727), (727, 728), (728, 729), (729, 730), (730, 731), (731, 732), (732, 733), (733, 734), (734, 735), (735, 736), (736, 737), (737, 738), (738, 739), (739, 740), (740, 741), (741, 742), (742, 743), (743, 744), (744, 745), (745, 746), (746, 747), (747, 748), (748, 749), (749, 750), (750, 751), (751, 752), (752, 753), (753, 754), (754, 755), (755, 756), (756, 757), (757, 758), (758, 759), (759, 760), (760, 761), (761, 762), (762, 763), (763, 767), (767, 768), (768, 769), (769, 770), (770, 772), (772, 773), (773, 774), (774, 775), (775, 776), (776, 777), (777, 778), (778, 779), (779, 780), (780, 781), (781, 782), (782, 783), (783, 785), (785, 786), (786, 787), (787, 788), (788, 789), (789, 790), (790, 791), (791, 792), (792, 793), (793, 795), (795, 796), (796, 797), (797, 798), (798, 799), (799, 800), (800, 801), (801, 802), (802, 803), (803, 804), (804, 805), (805, 806), (806, 807), (807, 808), (808, 809), (809, 810), (810, 811), (811, 812), (812, 813), (813, 814), (814, 815), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
      "384\n",
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "# 即使用滑动窗口分为多个文档，offset_mapping也还是按原始文档中的位置进行计算的\n",
    "print(tokenized_examples[\"offset_mapping\"][0]) \n",
    "print(tokenized_examples[\"offset_mapping\"][1])\n",
    "print(tokenized_examples[\"offset_mapping\"][2])\n",
    "print(len(tokenized_examples[\"offset_mapping\"][0]))\n",
    "print(len(tokenized_examples[\"offset_mapping\"][1]))\n",
    "print(len(tokenized_examples[\"offset_mapping\"][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0), (516, 517), (517, 518), (518, 519), (519, 520), (520, 521), (521, 522), (522, 523), (523, 524), (524, 525), (525, 526), (526, 527), (527, 528), (528, 529), (529, 530), (530, 531), (531, 532), (532, 533), (533, 534), (534, 535), (535, 536), (536, 537), (537, 538), (538, 539), (539, 540), (540, 541), (541, 542), (542, 543), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 552), (552, 553), (553, 554), (554, 555), (555, 557), (557, 558), (558, 559), (559, 560), (560, 561), (561, 562), (562, 563), (563, 564), (564, 565), (565, 566), (566, 567), (567, 568), (568, 569), (569, 570), (570, 571), (571, 572), (572, 573), (573, 574), (574, 575), (575, 576), (576, 577), (577, 578), (578, 579), (579, 580), (580, 581), (581, 582), (582, 583), (583, 584), (584, 585), (585, 586), (586, 587), (587, 588), (588, 589), (589, 590), (590, 591), (591, 592), (592, 593), (593, 594), (594, 595), (595, 596), (596, 597), (597, 598), (598, 599), (599, 600), (600, 601), (601, 603), (603, 604), (604, 606), (606, 607), (607, 608), (608, 609), (609, 610), (610, 611), (611, 612), (612, 613), (613, 614), (614, 615), (615, 616), (616, 617), (617, 618), (618, 619), (619, 620), (620, 621), (621, 622), (622, 623), (623, 624), (624, 625), (625, 626), (626, 627), (627, 631), (631, 632), (632, 633), (633, 637), (637, 638), (638, 639), (639, 640), (640, 641), (641, 642), (642, 643), (643, 644), (644, 645), (645, 646), (646, 647), (647, 648), (648, 649), (649, 650), (650, 651), (651, 652), (652, 653), (653, 657), (657, 658), (658, 659), (659, 660), (660, 662), (662, 663), (663, 664), (664, 665), (665, 666), (666, 667), (667, 668), (668, 669), (669, 670), (670, 671), (671, 672), (672, 673), (673, 674), (674, 675), (675, 676), (676, 677), (677, 678), (678, 679), (679, 680), (680, 681), (681, 682), (682, 683), (683, 684), (684, 685), (685, 686), (686, 687), (687, 688), (688, 689), (689, 690), (690, 691), (691, 692), (692, 693), (693, 694), (694, 695), (695, 696), (696, 697), (697, 698), (698, 699), (699, 700), (700, 701), (701, 702), (702, 703), (703, 704), (704, 705), (705, 706), (706, 707), (707, 708), (708, 709), (709, 710), (710, 714), (714, 715), (715, 716), (716, 717), (717, 719), (719, 720), (720, 721), (721, 722), (722, 723), (723, 724), (724, 725), (725, 726), (726, 727), (727, 728), (728, 729), (729, 730), (730, 731), (731, 732), (732, 733), (733, 734), (734, 735), (735, 736), (736, 737), (737, 738), (738, 739), (739, 740), (740, 741), (741, 742), (742, 743), (743, 744), (744, 745), (745, 746), (746, 747), (747, 748), (748, 749), (749, 750), (750, 751), (751, 752), (752, 753), (753, 754), (754, 755), (755, 756), (756, 757), (757, 758), (758, 759), (759, 760), (760, 761), (761, 762), (762, 763), (763, 767), (767, 768), (768, 769), (769, 770), (770, 772), (772, 773), (773, 774), (774, 775), (775, 776), (776, 777), (777, 778), (778, 779), (779, 780), (780, 781), (781, 782), (782, 783), (783, 785), (785, 786), (786, 787), (787, 788), (788, 789), (789, 790), (790, 791), (791, 792), (792, 793), (793, 795), (795, 796), (796, 797), (797, 798), (798, 799), (799, 800), (800, 801), (801, 802), (802, 803), (803, 804), (804, 805), (805, 806), (806, 807), (807, 808), (808, 809), (809, 810), (810, 811), (811, 812), (812, 813), (813, 814), (814, 815), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_examples[\"offset_mapping\"][2]) # offset_mapping显示的是token ids对应的token在原文档中的起始位置和结束位置！\n",
    "# 而word ids表示token ids对应的是第几个token，\n",
    "# 可以看到起始位置和结束位置基本上都相差1，这是因为中文基本上一个字就对应一个token\n",
    "# 还需要注意的是tokenizer自己加的特殊字符是不算位置的（标记为(0, 0)），且阅读理解问题和文档是分别算位置的，即使它们被拼起来了\n",
    "print(len(tokenized_examples[\"offset_mapping\"][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "len(sample_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 382 0 401 47 48\n",
      "token answer decode: 1963 年\n",
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 382 266 659 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 289 516 815 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 382 0 403 53 70\n",
      "token answer decode: 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 382 268 664 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 283 520 815 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 382 0 403 100 124\n",
      "token answer decode: 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 382 268 664 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 283 520 815 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 382 0 401 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 382 266 659 287 332\n",
      "token answer decode: 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 289 516 815 49 94\n",
      "token answer decode: 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 382 0 406 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 382 271 670 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 274 526 815 225 241\n",
      "token answer decode: 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 382 0 405 47 62\n",
      "token answer decode: 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 382 269 697 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 174 511 727 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 382 0 406 232 238\n",
      "token answer decode: 有 前 途 的 新 面 孔\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 382 270 699 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 171 513 727 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 382 0 406 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 382 270 699 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 171 513 727 155 170\n",
      "token answer decode: 《 jet 》 、 《 东 方 日 报 》 、 《 elle 》 等\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 382 0 408 205 207\n",
      "token answer decode: 售 货 员\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 382 272 703 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 165 517 727 0 0\n",
      "token answer decode: [CLS]\n",
      "{'text': ['大空翼'], 'answer_start': [84]} 84 87 21 382 0 370 105 107\n",
      "token answer decode: 大 空 翼\n",
      "{'text': ['大空翼'], 'answer_start': [84]} 84 87 21 252 234 501 0 0\n",
      "token answer decode: [CLS]\n"
     ]
    }
   ],
   "source": [
    "# 定位答案在token中的起始位置和结束位置\n",
    "for idx, _ in enumerate(sample_mapping):\n",
    "    # 先定位答案在原文档中的起始位置和结束位置。注意这段都用的是idx，没有涉及offset\n",
    "    answer = sample_dataset[\"answers\"][sample_mapping[idx]] # 注意这里不是idx而是sample_mapping[idx]，因为offset_mapping还是原文档的\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "    # 下面要定位答案在各个文档token中的起始位置和结束位置，从各个文档token中context的起始和结束向答案逼近。\n",
    "\n",
    "    # 注意这里不能用token_type_ids寻找context的起始和结束位置所对应的token ids，因为它会把特殊字符也当成0或1\n",
    "    # 要用sequence_ids，其中特殊字符用None表示，第一句话用0第二句话用1标注\n",
    "    # 注意这里不是sample_mapping[idx]而是idx，因为要在各个文档中都找一遍答案！\n",
    "    context_start = tokenized_examples.sequence_ids(idx).index(1) # 找到第一个1，即context的起始位置对应的token\n",
    "    # 找到第一个1后的第一个None，然后它的前面那个就是最后一个1，即context的结束位置对应的token\n",
    "    context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "\n",
    "    offset = tokenized_examples.get(\"offset_mapping\")[idx] # idx本来就应该与offset一一对应\n",
    "\n",
    "    # 判断答案是否在context中，根据各个文档token中的起始位置和结束位置在原文档中的位置\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char: # 比较的两个都是在原文档中的位置\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else: # 逐个查找答案在各个文档token中的起始位置和结束位置，由于截断可能也只有部分答案\n",
    "        token_id = context_start\n",
    "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "            token_id += 1\n",
    "        start_token_pos = token_id\n",
    "        token_id = context_end\n",
    "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "            token_id -=1\n",
    "        end_token_pos = token_id\n",
    "    \n",
    "    # 显然会出现offset[context_start][0]小于context_start和offset[context_end][1]大于context_end的情况，\n",
    "    # 因为tokenizer可能会将多个汉字编码成一个token，却很少把一个汉字编码成多个tokens\n",
    "    # 但tokenizer可能将多个英语单词编码成一个token（不太常见），也可能将一个英语单词编码成多个tokens（常见）\n",
    "    # start_char, end_char, offset[context_start][0], offset[context_end][1]：原文档中的位置\n",
    "    # context_start, context_end, start_token_pos, end_token_pos：token中的位置\n",
    "    print(answer, start_char, end_char, context_start, context_end, offset[context_start][0], offset[context_end][1], start_token_pos, end_token_pos)\n",
    "    print(\"token answer decode:\", tokenizer.decode(tokenized_examples[\"input_ids\"][idx][start_token_pos: end_token_pos + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_examples.sequence_ids(idx))\n",
    "print(len(tokenized_examples.sequence_ids(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples): # 主要还是制作标签\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                               text_pair=examples[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=384, truncation=\"only_second\", padding=\"max_length\")\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    for idx, _ in enumerate(sample_mapping):\n",
    "        # 先定位答案在原文档中的起始位置和结束位置。注意这段都用的是idx，没有涉及offset\n",
    "        answer = examples[\"answers\"][sample_mapping[idx]] # 注意这里不是idx而是sample_mapping[idx]，因为offset_mapping还是原文档的\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        # 下面要定位答案在各个文档token中的起始位置和结束位置，从各个文档token中context的起始和结束向答案逼近。\n",
    "\n",
    "        # 注意这里不能用token_type_ids寻找context的起始和结束位置所对应的token ids，因为它会把特殊字符也当成0或1\n",
    "        # 要用sequence_ids，其中特殊字符用None表示，第一句话用0第二句话用1标注\n",
    "        # 注意这里不是sample_mapping[idx]而是idx，因为要在各个文档中都找一遍答案！\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1) # 找到第一个1，即context的起始位置对应的token\n",
    "        # 找到第一个1后的第一个None，然后它的前面那个就是最后一个1，即context的结束位置对应的token\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "\n",
    "        offset = tokenized_examples.get(\"offset_mapping\")[idx] # idx本来就应该与offset一一对应\n",
    "\n",
    "        # 判断答案是否在context中，根据各个文档token中的起始位置和结束位置在原文档中的位置\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char: # 比较的两个都是在原文档中的位置\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else: # 逐个查找答案在各个文档token中的起始位置和结束位置，由于截断可能也只有部分答案\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -=1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_mapping[idx]]) # 这是滑动窗口映射到原文档的编码！而overflow_to_sample_mapping只是映射到原文档的下标\n",
    "        tokenized_examples[\"offset_mapping\"][idx] = [\n",
    "            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][idx])\n",
    "        ] # offset_mapping就不要留着特殊字符和question的部分了都设成None，只保留context；\n",
    "          # 这是为了配合预测，也就是如果输出映射到offset_mapping是None那就等于没找到，可以直接判断了\n",
    "        \n",
    "    tokenized_examples[\"example_ids\"] = example_ids\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 19189\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 6327\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1988\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, [266, 267], [267, 268], [268, 269], [269, 270], [270, 271], [271, 272], [272, 273], [273, 274], [274, 275], [275, 276], [276, 277], [277, 278], [278, 279], [279, 280], [280, 281], [281, 282], [282, 283], [283, 284], [284, 285], [285, 286], [286, 287], [287, 288], [288, 289], [289, 290], [290, 291], [291, 292], [292, 293], [293, 294], [294, 295], [295, 296], [296, 297], [297, 298], [298, 299], [299, 300], [300, 301], [301, 302], [302, 303], [303, 304], [304, 305], [305, 306], [306, 307], [307, 308], [308, 309], [309, 310], [310, 311], [311, 312], [312, 313], [313, 314], [314, 315], [315, 316], [316, 317], [317, 318], [318, 319], [319, 320], [320, 321], [321, 325], [325, 326], [326, 327], [327, 328], [328, 329], [329, 330], [330, 331], [331, 332], [332, 333], [333, 334], [334, 335], [335, 336], [336, 337], [337, 338], [338, 339], [339, 340], [340, 341], [341, 342], [342, 343], [343, 344], [344, 345], [345, 346], [346, 347], [347, 348], [348, 349], [349, 350], [350, 351], [351, 352], [352, 353], [353, 354], [354, 355], [355, 356], [356, 360], [360, 361], [361, 362], [362, 363], [363, 364], [364, 365], [365, 366], [366, 367], [367, 368], [368, 369], [369, 370], [370, 371], [371, 372], [372, 373], [373, 374], [374, 375], [375, 376], [376, 377], [377, 378], [378, 379], [379, 380], [380, 381], [381, 382], [382, 383], [383, 384], [384, 385], [385, 386], [386, 387], [387, 388], [388, 390], [390, 391], [391, 392], [392, 393], [393, 394], [394, 395], [395, 396], [396, 397], [397, 398], [398, 399], [399, 400], [400, 401], [401, 402], [402, 403], [403, 404], [404, 405], [405, 406], [406, 407], [407, 408], [408, 409], [409, 410], [410, 411], [411, 412], [412, 413], [413, 414], [414, 415], [415, 416], [416, 417], [417, 418], [418, 419], [419, 420], [420, 421], [421, 422], [422, 424], [424, 425], [425, 426], [426, 427], [427, 428], [428, 429], [429, 430], [430, 431], [431, 432], [432, 433], [433, 434], [434, 435], [435, 436], [436, 437], [437, 438], [438, 439], [439, 440], [440, 441], [441, 442], [442, 443], [443, 444], [444, 445], [445, 446], [446, 447], [447, 448], [448, 449], [449, 450], [450, 451], [451, 452], [452, 453], [453, 454], [454, 455], [455, 456], [456, 457], [457, 458], [458, 459], [459, 460], [460, 461], [461, 462], [462, 463], [463, 464], [464, 465], [465, 466], [466, 467], [467, 468], [468, 469], [469, 470], [470, 471], [471, 472], [472, 473], [473, 474], [474, 475], [475, 476], [476, 477], [477, 478], [478, 479], [479, 480], [480, 481], [481, 482], [482, 483], [483, 484], [484, 485], [485, 486], [486, 487], [487, 488], [488, 489], [489, 490], [490, 491], [491, 492], [492, 493], [493, 494], [494, 495], [495, 499], [499, 500], [500, 501], [501, 502], [502, 503], [503, 504], [504, 505], [505, 506], [506, 507], [507, 508], [508, 509], [509, 510], [510, 511], [511, 512], [512, 513], [513, 514], [514, 516], [516, 517], [517, 518], [518, 519], [519, 520], [520, 521], [521, 522], [522, 523], [523, 524], [524, 525], [525, 526], [526, 527], [527, 528], [528, 529], [529, 530], [530, 531], [531, 532], [532, 533], [533, 534], [534, 535], [535, 536], [536, 537], [537, 538], [538, 539], [539, 540], [540, 541], [541, 542], [542, 543], [543, 544], [544, 545], [545, 546], [546, 547], [547, 548], [548, 552], [552, 553], [553, 554], [554, 555], [555, 557], [557, 558], [558, 559], [559, 560], [560, 561], [561, 562], [562, 563], [563, 564], [564, 565], [565, 566], [566, 567], [567, 568], [568, 569], [569, 570], [570, 571], [571, 572], [572, 573], [573, 574], [574, 575], [575, 576], [576, 577], [577, 578], [578, 579], [579, 580], [580, 581], [581, 582], [582, 583], [583, 584], [584, 585], [585, 586], [586, 587], [587, 588], [588, 589], [589, 590], [590, 591], [591, 592], [592, 593], [593, 594], [594, 595], [595, 596], [596, 597], [597, 598], [598, 599], [599, 600], [600, 601], [601, 603], [603, 604], [604, 606], [606, 607], [607, 608], [608, 609], [609, 610], [610, 611], [611, 612], [612, 613], [613, 614], [614, 615], [615, 616], [616, 617], [617, 618], [618, 619], [619, 620], [620, 621], [621, 622], [622, 623], [623, 624], [624, 625], [625, 626], [626, 627], [627, 631], [631, 632], [632, 633], [633, 637], [637, 638], [638, 639], [639, 640], [640, 641], [641, 642], [642, 643], [643, 644], [644, 645], [645, 646], [646, 647], [647, 648], [648, 649], [649, 650], [650, 651], [651, 652], [652, 653], [653, 657], [657, 658], [658, 659], None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][\"offset_mapping\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRAIN_186_QUERY_0',\n",
       " 'TRAIN_186_QUERY_0',\n",
       " 'TRAIN_186_QUERY_0',\n",
       " 'TRAIN_186_QUERY_1',\n",
       " 'TRAIN_186_QUERY_1',\n",
       " 'TRAIN_186_QUERY_1',\n",
       " 'TRAIN_186_QUERY_2',\n",
       " 'TRAIN_186_QUERY_2',\n",
       " 'TRAIN_186_QUERY_2',\n",
       " 'TRAIN_186_QUERY_3']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][\"example_ids\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'TRAIN_186_QUERY_0': [0, 1, 2],\n",
       "             'TRAIN_186_QUERY_1': [3, 4, 5],\n",
       "             'TRAIN_186_QUERY_2': [6, 7, 8],\n",
       "             'TRAIN_186_QUERY_3': [9]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "# 原文档的编码映射到滑动窗口\n",
    "example_to_feature = collections.defaultdict(list) # 如果尝试访问defaultdict中不存在的键，defaultdict默认会自动为该键创建对应的值：空列表\n",
    "for idx, example_id in enumerate(tokenized_datasets[\"train\"][\"example_ids\"][:10]):\n",
    "    example_to_feature[example_id].append(idx)\n",
    "example_to_feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 获取模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# 提取最佳预测结果和相应的标签\n",
    "def get_result(start_logits, end_logits, exmaples, features): \n",
    "    # start_logits和end_logits的形状都是[batch_size, seq_length]，包括特殊字符、question和context\n",
    "    # exmaples表示原始数据，features就是每个滑动窗口\n",
    "\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    # 原文档的编码映射到滑动窗口\n",
    "    example_to_feature = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_feature[example_id].append(idx)\n",
    "\n",
    "    # 最优答案候选\n",
    "    n_best = 20\n",
    "    # 最大答案长度\n",
    "    max_answer_length = 30\n",
    "\n",
    "    for example in exmaples: # 对原文档逐个进行筛选\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        for feature_idx in example_to_feature[example_id]: # 对于原文档对应的所有滑动窗口独立筛选\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist() # np.argsort返回的不是值而是下标\n",
    "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 先排除不可能的答案\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[start_index][0]: offset[end_index][1]], # 原文档中的起始和结束位置\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index] # 当前预测的分数之和\n",
    "                        # 为什么不直接选start_logit最大的和end_logit最大的？其实是因为要先排除掉不可能的答案！\n",
    "                    })\n",
    "\n",
    "        if len(answers) > 0:\n",
    "            # 所有滑动窗口的可行答案中取分数之和最高的！\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else: # 所有滑动窗口都没有取到。。。\n",
    "            predictions[example_id] = \"\"\n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "\n",
    "    return predictions, references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmrc_eval import evaluate_cmrc # 用了cmrc_eval.py中的evaluate_cmrc函数\n",
    "\n",
    "def metirc(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenized_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenized_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenized_datasets[\"test\"])\n",
    "    # 评价指标包括精准匹配度EM（看是否完全相等）和模糊匹配度F1（利用最长公共子串计算Precision和Recall最终合成F1）\n",
    "    return evaluate_cmrc(p, r) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /data/PLM/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"/data/PLM/chinese-macbert-base\")\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_qa\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200, # 每200步evaluate一次！\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 配置Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(), # 主要也是为了分批处理，其他上面都已经完成了，batch size见args = TrainingArguments\n",
    "    compute_metrics=metirc\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 30:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Avg</th>\n",
       "      <th>F1</th>\n",
       "      <th>Em</th>\n",
       "      <th>Total</th>\n",
       "      <th>Skip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.519200</td>\n",
       "      <td>1.348136</td>\n",
       "      <td>75.856872</td>\n",
       "      <td>85.575191</td>\n",
       "      <td>66.138552</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.418000</td>\n",
       "      <td>1.194390</td>\n",
       "      <td>77.344989</td>\n",
       "      <td>86.407903</td>\n",
       "      <td>68.282075</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.328300</td>\n",
       "      <td>1.135692</td>\n",
       "      <td>78.387408</td>\n",
       "      <td>87.529709</td>\n",
       "      <td>69.245107</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.255299</td>\n",
       "      <td>77.225157</td>\n",
       "      <td>86.789549</td>\n",
       "      <td>67.660764</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.007700</td>\n",
       "      <td>1.201607</td>\n",
       "      <td>79.023106</td>\n",
       "      <td>87.465286</td>\n",
       "      <td>70.580926</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.038300</td>\n",
       "      <td>1.206736</td>\n",
       "      <td>76.193098</td>\n",
       "      <td>85.812725</td>\n",
       "      <td>66.573470</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>1.315936</td>\n",
       "      <td>77.517120</td>\n",
       "      <td>86.783231</td>\n",
       "      <td>68.251010</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.767900</td>\n",
       "      <td>1.381830</td>\n",
       "      <td>75.235335</td>\n",
       "      <td>85.326215</td>\n",
       "      <td>65.144455</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.774300</td>\n",
       "      <td>1.355522</td>\n",
       "      <td>76.278532</td>\n",
       "      <td>85.952529</td>\n",
       "      <td>66.604536</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1800, training_loss=1.1303160285949707, metrics={'train_runtime': 1852.3397, 'train_samples_per_second': 31.078, 'train_steps_per_second': 0.972, 'total_flos': 1.1281552796265984e+16, 'train_loss': 1.1303160285949707, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x7f6b950cf7f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9453522562980652, 'start': 3, 'end': 5, 'answer': '北京'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
