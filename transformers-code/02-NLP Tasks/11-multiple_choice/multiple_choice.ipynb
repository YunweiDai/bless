{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformers的多项选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与机器阅读理解的不同之处在于输入是先给出context，再接上question和choice，有三个部分。context与question之间、choice最后有[SEP]分隔符，但question和choice可以只用空格分隔。此外有几个choice就构造几个输入，最后将[CLS]通过输出维度为1的全连接层，并拼接在一起做softmax取最大值，因为这说说是多项选择但答案只有一个，所以需要区分哪个选项最好。。。改进可能在trick上有一些修改，但本质还是这套"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import DatasetDict # 数据集已经明确划分训练、验证和测试，就需要用DatasetDict（可以自己做一个dataset_dict.json）\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3 = DatasetDict.load_from_disk(\"./c3/\") # 必须是save_to_disk保存下来的数据集才能这么读取\n",
    "c3 # 三个包含'id'、'context'、'question'、'choice'和'answer'键及对应值的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 'context': [['男：你今天晚上有时间吗?我们一起去看电影吧?', '女：你喜欢恐怖片和爱情片，但是我喜欢喜剧片，科幻片一般。所以……'],\n",
       "  ['男：足球比赛是明天上午八点开始吧?', '女：因为天气不好，比赛改到后天下午三点了。'],\n",
       "  ['女：今天下午的讨论会开得怎么样?', '男：我觉得发言的人太少了。'],\n",
       "  ['男：我记得你以前很爱吃巧克力，最近怎么不吃了，是在减肥吗?', '女：是啊，我希望自己能瘦一点儿。'],\n",
       "  ['女：过几天刘明就要从英国回来了。我还真有点儿想他了，记得那年他是刚过完中秋节走的。',\n",
       "   '男：可不是嘛!自从我去日本留学，就再也没见过他，算一算都五年了。',\n",
       "   '女：从2000年我们在学校第一次见面到现在已经快十年了。我还真想看看刘明变成什么样了!',\n",
       "   '男：你还别说，刘明肯定跟英国绅士一样，也许还能带回来一个英国女朋友呢。'],\n",
       "  ['男：好久不见了，最近忙什么呢?',\n",
       "   '女：最近我们单位要搞一个现代艺术展览，正忙着准备呢。',\n",
       "   '男：你们不是出版公司吗?为什么搞艺术展览?',\n",
       "   '女：对啊，这次展览是我们出版的一套艺术丛书的重要宣传活动。'],\n",
       "  ['男：会议结束后，你记得把空调和灯都关了。', '女：好的，我知道了，明天见。'],\n",
       "  ['男：你出国读书的事定了吗?', '女：思前想后，还拿不定主意呢。'],\n",
       "  ['男：这件衣服我要了，在哪儿交钱?', '女：前边右拐就有一个收银台，可以交现金，也可以刷卡。'],\n",
       "  ['男：小李啊，你是我见过的最爱干净的学生。',\n",
       "   '女：谢谢教授夸奖。不过，您是怎么看出来的?',\n",
       "   '男：不管我叫你做什么，你总是推得干干净净。',\n",
       "   '女：教授，我……']],\n",
       " 'question': ['女的最喜欢哪种电影?',\n",
       "  '根据对话，可以知道什么?',\n",
       "  '关于这次讨论会，我们可以知道什么?',\n",
       "  '女的为什么不吃巧克力了?',\n",
       "  '现在大概是哪一年?',\n",
       "  '女的的公司为什么要做现代艺术展览?',\n",
       "  '他们最可能是什么关系?',\n",
       "  '女的是什么意思?',\n",
       "  '他们最可能在什么地方?',\n",
       "  '教授认为小李怎么样?'],\n",
       " 'choice': [['恐怖片', '爱情片', '喜剧片', '科幻片'],\n",
       "  ['今天天气不好', '比赛时间变了', '校长忘了时间'],\n",
       "  ['会是昨天开的', '男的没有参加', '讨论得不热烈', '参加的人很少'],\n",
       "  ['刷牙了', '要减肥', '口渴了', '吃饱了'],\n",
       "  ['2005年', '2010年', '2008年', '2009年'],\n",
       "  ['传播文化', '宣传新书', '推广现代艺术', '体现企业文化'],\n",
       "  ['同事', '司机和客人', '医生和病人'],\n",
       "  ['不想出国', '出国太难', '还在犹豫', '不想决定'],\n",
       "  ['医院', '迪厅', '商场', '饭馆'],\n",
       "  ['卫生习惯非常好', '做事的能力不够', '找借口拒绝做事', '记不住该做的事']],\n",
       " 'answer': ['喜剧片',\n",
       "  '比赛时间变了',\n",
       "  '讨论得不热烈',\n",
       "  '要减肥',\n",
       "  '2010年',\n",
       "  '宣传新书',\n",
       "  '同事',\n",
       "  '还在犹豫',\n",
       "  '商场',\n",
       "  '找借口拒绝做事']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[\"train\"][:10] # 如果只有1个的话'id'、'question'和'answer'键对应的值会是字符串而不是列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "    num_rows: 1625\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3.pop(\"test\") # \"test\"部分的'answer'是空的，所以需要先把它去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='/data/PLM/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/PLM/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    # examples, dict, keys: [\"context\", \"quesiton\", \"choice\", \"answer\"]\n",
    "    # examples, 1000\n",
    "    context = []\n",
    "    question_choice = []\n",
    "    labels = []\n",
    "    for idx in range(len(examples[\"context\"])):\n",
    "        ctx = \"\\n\".join(examples[\"context\"][idx]) # context是对话形式，所以需要先拼接一下\n",
    "        question = examples[\"question\"][idx]\n",
    "        choices = examples[\"choice\"][idx]\n",
    "        for choice in choices:\n",
    "            context.append(ctx) # 每个choice都对应相同的question和context，这样tokenizer的时候才能一一对应起来！！\n",
    "            question_choice.append(question + \" \" + choice) # question和choice之间还是最好加一个空格以区分，也可以用其他的\n",
    "        if len(choices) < 4:\n",
    "            for _ in range(4 - len(choices)): # 不是每个问题都有4个选项，但最好还是填充成4个，因为最后要把[CLS]的结果拼接起来\n",
    "                context.append(ctx)\n",
    "                question_choice.append(question + \" \" + \"以上都是\") # 填充的选项倒是无所谓\n",
    "        labels.append(choices.index(examples[\"answer\"][idx])) # labels处理成一个数字，第几个选项\n",
    "    tokenized_examples = tokenizer(context, question_choice, truncation=\"only_first\", max_length=256, padding=\"max_length\") # 每个键对应的值都是4000 * 256的嵌套列表,\n",
    "    # 截断还是只截context吧！question和choice显然更关键！\n",
    "    tokenized_examples = {k: [v[i: i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()} # 每个键对应的值都是1000 * 4 * 256的嵌套列表\n",
    "    # tokenized_examples.items()要遍历也只有遍历键和值了！这样一来每个sample都包含4 * 256的input_ids等\n",
    "    tokenized_examples[\"labels\"] = labels # 笑死，最后再加上labels，正好“一一对应”\n",
    "    return tokenized_examples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'choice', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c3[\"train\"].select(range(10)).map(process_function, batched=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(res[\"input_ids\"]).shape # 转化为array就能看形状了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_c3 = c3.map(process_function, batched=True)\n",
    "tokenized_c3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at /data/PLM/chinese-macbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(\"/data/PLM/chinese-macbert-base\")\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"/data/daiyw/Compare/evaluate/metrics/accuracy\")\n",
    "\n",
    "def compute_metric(pred):\n",
    "    predictions, labels = pred # 输出的第一个是预测，第二个才是标签！！！\n",
    "    predictions = np.argmax(predictions, axis=-1) # 输出是array格式，必须用numpy才能处理\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./muliple_choice\",\n",
    "    per_device_train_batch_size=16, # 一个sample要承担原来4倍的工作量，所以batch size其实可以视为64！\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True # 多了fp16的部分，这是因为batch size实质上扩大到原来的4倍，需要尽可能减少显存占用！\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_c3[\"train\"],\n",
    "    eval_dataset=tokenized_c3[\"validation\"],\n",
    "    # data_collator=DefaultDataCollator(), # 其实不需要？最重要的批处理功能在args = TrainingArguments的batch参数设置中已经暗示了\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2226' max='2226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2226/2226 12:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.961100</td>\n",
       "      <td>0.925483</td>\n",
       "      <td>0.603249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>0.632862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>1.296424</td>\n",
       "      <td>0.647013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2226, training_loss=0.6738789636384016, metrics={'train_runtime': 774.8703, 'train_samples_per_second': 45.952, 'train_steps_per_second': 2.873, 'total_flos': 1.873702246273229e+16, 'train_loss': 0.6738789636384016, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "\n",
    "class MultipleChoicePipeline:\n",
    "\n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def preprocess(self, context, quesiton, choices):\n",
    "        cs, qcs = [], []\n",
    "        for choice in choices:\n",
    "            cs.append(context)\n",
    "            qcs.append(quesiton + \" \" + choice)\n",
    "        return tokenizer(cs, qcs, truncation=\"only_first\", max_length=256, return_tensors=\"pt\") # 返回tensor格式\n",
    "\n",
    "    def predict(self, inputs):\n",
    "         # 本来v只有[num_choice, seq_length]的形状，这里要用unsqueeze加上batch_size的维度\n",
    "        inputs = {k: v.unsqueeze(0).to(self.device) for k, v in inputs.items()} # 加载到GPU上居然可以只加载键的值？？自己做的时候还是全部加载吧\n",
    "        return self.model(**inputs).logits\n",
    "\n",
    "    def postprocess(self, logits, choices):\n",
    "        predition = torch.argmax(logits, dim=-1).cpu().item()\n",
    "        return choices[predition]\n",
    "\n",
    "    def __call__(self, context, question, choices) -> Any: # 像pipeline这样声明对象后可以直接当函数用的类需要__call__函数\n",
    "        inputs = self.preprocess(context, question, choices)\n",
    "        logits = self.predict(inputs)\n",
    "        result = self.postprocess(logits, choices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = MultipleChoicePipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北京'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试时num_choice可以不固定，甚至choice可以重复，这就是将[CLS]通过输出维度为1的全连接层，并拼接在一起做softmax取最大值进行分类的好处，不会受到choice个数的限制！\n",
    "pipe(context = \"小明在北京上班\", question = \"小明在哪里上班？\", choices = [\"北京\", \"上海\", \"河北\", \"海南\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
