{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于GLM的文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"./nlpcc_2017/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 4900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.train_test_split(100, seed=42)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '组图:黑河边防军人零下30℃户外训练,冰霜沾满眉毛和睫毛,防寒服上满是冰霜。',\n",
       " 'content': '中国军网2014-12-1709:08:0412月16日,黑龙江省军区驻黑河某边防团机动步兵连官兵,冒着-30℃严寒气温进行体能训练,挑战极寒,锻造钢筋铁骨。该连素有“世界冠军的摇篮”之称,曾有5人24人次登上世界军事五项冠军的领奖台。(魏建顺摄)黑龙江省军区驻黑河某边防团机动步兵连官兵冒着-30℃严寒气温进行体能训练驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜官兵睫毛上都被冻上了冰霜官兵们睫毛上都被冻上了冰霜驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GLMChineseTokenizer(name_or_path='/data/PLM/glm-large-chinese', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|endoftext|>', 'unk_token': '[UNK]', 'pad_token': '<|endoftext|>', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<|startofpiece|>', '<|endofpiece|>', '[gMASK]', '[sMASK]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50001: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50002: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50003: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50004: AddedToken(\"[UNUSED1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50005: AddedToken(\"[UNUSED2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50006: AddedToken(\"<|startofpiece|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50007: AddedToken(\"<|endofpiece|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50008: AddedToken(\"[sMASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50009: AddedToken(\"[gMASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50010: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/PLM/glm-large-chinese\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(exmaples):\n",
    "    contents = [\"摘要生成: \\n\" + e + tokenizer.mask_token for e in exmaples[\"content\"]] # 除了prefix，还要再最后加上tokenizer.mask_token！\n",
    "    inputs = tokenizer(contents, max_length=384, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    # glm特殊的处理labels的方式\n",
    "    inputs = tokenizer.build_inputs_for_generation(inputs, targets=exmaples['title'], padding=True, max_gen_length=64)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4900 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4900/4900 [00:58<00:00, 83.96 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:01<00:00, 84.54 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'position_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'position_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds[\"train\"].column_names) # remove_columns会去掉原本的'title', 'content'列，转为'input_ids', 'position_ids', 'attention_mask', 'labels'\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 摘要生成: 中国军网2014-12-1709:08:0412月16日,黑龙江省军区驻黑河某边防团机动步兵连官兵,冒着-30°C严寒气温进行体能训练,挑战极寒,锻造钢筋铁骨。该连素有“世界冠军的摇篮”之称,曾有5人24人次登上世界军事五项冠军的领奖台。(魏建顺摄)黑龙江省军区驻黑河某边防团机动步兵连官兵冒着-30°C严寒气温进行体能训练驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜驻黑河某边防团机动步兵连官兵严寒中户外训练,防寒服上满是冰霜官兵睫毛上都被冻上了冰霜官兵们睫毛上都被冻上了冰霜驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练驻黑河某边防团机动步兵连官兵严寒中进行户外体能训练[MASK]<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> <|startofpiece|> 组图:黑河边防军人零下30°C户外训练,冰霜沾满眉毛和睫毛,防寒服上满是冰霜。 <|endofpiece|> <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[\"train\"][0][\"input_ids\"]) # 注意input_ids的开始是[CLS]，但labels的开始是<|startofpiece|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 37275, 43736, 43383, 43979, 25079, 44010, 8740, 41929, 34437, 43573, 4620, 995, 43359, 44508, 45333, 46056, 43993, 15210, 43384, 15022, 43359, 44010, 44773, 43674, 43387, 32058, 44508, 45333, 43361, 50007, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[\"train\"][0][\"labels\"]) # 直接decode会因为-100而报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50002, 43358, 23217, 4490, 43383, 576, 43790, 43593, 1251, 2979, 10422, 1902, 43383, 1976, 43383, 2638, 64, 43491, 195, 43498, 43359, 12929, 9872, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 43359, 25716, 6905, 44801, 43573, 39991, 5316, 74, 20977, 995, 43359, 2265, 44003, 44773, 43359, 29329, 13922, 44210, 44394, 43361, 43655, 43905, 21178, 43430, 91, 1534, 43360, 23052, 43432, 12292, 43359, 31750, 43402, 43371, 369, 15386, 11946, 91, 2227, 37620, 1534, 43360, 43952, 44069, 43820, 3700, 45176, 43555, 44302, 44415, 43396, 12929, 9872, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 25716, 6905, 44801, 43573, 39991, 5316, 74, 20977, 995, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 39991, 43378, 4620, 995, 43359, 44010, 44773, 43674, 43387, 32058, 44508, 45333, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 39991, 43378, 4620, 995, 43359, 44010, 44773, 43674, 43387, 32058, 44508, 45333, 17586, 15022, 43387, 4820, 45418, 1035, 44508, 45333, 17586, 43457, 15022, 43387, 4820, 45418, 1035, 44508, 45333, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 39991, 27131, 4620, 20977, 995, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 39991, 27131, 4620, 20977, 995, 45218, 43979, 43965, 44221, 31855, 43895, 4828, 9404, 43905, 17586, 39991, 27131, 4620, 20977, 995, 50003, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50006, 37275, 43736, 43383, 43979, 25079, 44010, 8740, 41929, 34437, 43573, 4620, 995, 43359, 44508, 45333, 46056, 43993, 15210, 43384, 15022, 43359, 44010, 44773, 43674, 43387, 32058, 44508, 45333, 43361, 50007, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000]\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]]\n"
     ]
    }
   ],
   "source": [
    "# input_ids还比较正常：\n",
    "# 50002是[CLS]，表示inputs的开始；50003是[MASK]；50000是<|endoftext|>，表示inputs的结束，也作为inputs和labels的[PAD]使用；\n",
    "# 50006是<|startofpiece|>，表示labels的开始；50007是<|endofpiece|>，表示表示labels的结束；需要注意的是50006不算在labels中！\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])\n",
    "# 位置编码去看下GLM的原理就能清楚：input_ids部分前面是正常的编码，最后接上[MASK]位置的编码重复max_gen_length次；\n",
    "# labels部分前面是0的重复，最后接上正常的编码直到max_gen_length\n",
    "print(tokenized_ds[\"train\"][0][\"position_ids\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/data/PLM/glm-large-chinese\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLMForConditionalGeneration(\n",
       "  (glm): GLMModel(\n",
       "    (word_embeddings): VocabEmbedding()\n",
       "    (transformer): GLMStack(\n",
       "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(1025, 1024)\n",
       "      (block_position_embeddings): Embedding(1025, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x GLMBlock(\n",
       "          (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # GLM是仅编码器结构，但不是encoder-only而是prefix-lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 配置训练参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glm不太适合在训练时评估，所以这里就没有compute_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary_glm\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=8,\n",
    "    num_train_epochs=1 # 注意默认值为3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer) # 主要是做padding，但上面已经做好了\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 16:36, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.598700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=153, training_loss=1.6627078874438417, metrics={'train_runtime': 1003.8592, 'train_samples_per_second': 4.881, 'train_steps_per_second': 0.152, 'total_flos': 4653015575298048.0, 'train_loss': 1.6627078874438417, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50007 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 摘要生成: 中国证券网讯(记者严政)中国重工6月14日晚间公告称,公司日前接到控股股东中国船舶重工集团公司(简称“中船重工”)通知,中船重工拟对其自身相关业务进行调整,部分业务涉及到公司。其中,公司拟以持有的动力相关资产进行对外投资,参与中船重工拟打造的动力业务平台公司。公司上述对外投资的方案不构成重大资产重组,也不涉及公司发行股份。中国重工表示,目前方案还需进一步论证,存在不确定性。为保证公平信息披露,维护投资者利益,避免造成公司股价异常波动,经公司申请,自6月12日下午开市起公司股票停牌。同时公司将与中船重工保持密切联系,尽快确认是否进行上述事项,并于股票停牌之日起5个工作日内(含停牌当日)公告事项进展情况。[MASK]<|endoftext|> <|startofpiece|> 中国重工公告称,接到中船重工通知,公司拟以持有的动力资产进行对外投资参与中船重工拟打造的动力业务平台公司,目前方案需进一步论证;公司股将于12日下午开市起停牌。 <|endofpiece|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = ds[\"test\"][-1][\"content\"]\n",
    "inputs = tokenizer(\"摘要生成: \\n\" + input_text + tokenizer.mask_token, return_tensors=\"pt\")\n",
    "inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=64)\n",
    "inputs = inputs.to(\"cuda\")\n",
    "output = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eop_token_id, do_sample=True) # eos_token_id必须设置成eop_token_id而不是eos_token_id\n",
    "tokenizer.decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "def predict_test():\n",
    "    predict = []\n",
    "    with torch.inference_mode():\n",
    "        for d in ds[\"test\"]:\n",
    "            inputs = tokenizer(\"摘要生成: \\n\" + d[\"content\"] + tokenizer.mask_token, return_tensors=\"pt\")\n",
    "            # 这里也要进行修改，而不是简单地用tokenizer，position_ids也要这一步才有！\n",
    "            inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=64) # 会在最后的[MASK]和<endoftext>后加<|startofpiece|>(50006)\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            # tokenizer.eop_token_id其实就是50007\n",
    "            output = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eop_token_id, do_sample=True)\n",
    "            predict.append(tokenizer.decode(output[0].tolist()).split(\"<|startofpiece|>\")[1].replace(\"<|endofpiece|>\", \"\").strip())\n",
    "            print(\"curID:\", len(predict))\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['媒体称IS公布1400名西方政界人士名单,包括美国国务院和国防部人员,其中不乏美国政界重要人士,其威胁将杀1400人。',\n",
       " '宿松县2公职人员吸毒被开除党籍、行政撤职,县纪委通报:全县党政机关一把手要引以为戒。',\n",
       " '媒体称黑龙江省“伪基站”犯罪数量已超100万户,每天影响手机用户达190万个。',\n",
       " '北京明天实行机动车尾号轮换,周一至周五限行2和7!',\n",
       " '今日10时许,松北宾馆门前一对母子打成一团,母亲怒撕儿子上衣;警方随后介入后,三人和好离开现场',\n",
       " '苏州吴江一初中副校长泄考题被停职:泄题人系副校长亲戚,题目自己先泄露给亲戚,随后将其散播,导致题目传至网上。',\n",
       " '承德广播电视台2名处级干部严重违纪被开除党籍、开除公职,其涉嫌犯罪问题及线索移送司法机关查处,目前正进一步办理中',\n",
       " '曝李帅佩斯微博首秀写汉字,网友:真佩服这大汉,字间距堪比尺子',\n",
       " '调查表明,穆沙拉夫称“伊斯兰国”控制了至少两个主要小麦产区,已攫取100多万吨小麦,占伊拉克年消费量的1/5',\n",
       " '辽宁省气象台发布大风黄色预警:预计今日夜间到明天白天渤海海面,将有西北风9级,阵风10-11级。请有关单位和人员作好防范准备...',\n",
       " '上海市委书记会见缅甸全国民主联盟主席,介绍浦东发展情况;缅甸称上海是发展最快经济体之一。中联部副部长、上海市委秘书长尹弘参加会见。',\n",
       " '湖南凤凰有位122岁老寿星,一生生育13个孩子,最小的仅活至18岁,被评为“湖南省十大寿星之首”。',\n",
       " '杭州拱墅区大关街道正科级干部包某被举报涉嫌强奸他人,已被开除党籍、行政撤职。',\n",
       " '腰缠万贯”藏腰间逃法海 女子藏16斤金条闯海关',\n",
       " '武汉一男大学生摆摊招聘女友,称想找女友无需太夸张,其前女友曾在该校招聘广告上留下QQ号码',\n",
       " '荆州市发布霾黄色预警:目前我市部分地区已经出现能见度小于5公里的霾,未来仍将持续,请注意防范。...',\n",
       " '[拌饭]成龙广告代言受辱,曝广告台词被篡改成“龙爪手”;朱孝天晒拼图疑公布恋情;大s晒照为好友庆祝生仔',\n",
       " '山东莱州农民屋顶建50千瓦光伏发电系统,获得国家政策支持,平均每度电上网电价1元左右',\n",
       " '晋中:姐姐失踪4天,家人四处寻找弟弟,孩子无外伤,警方正寻人',\n",
       " '5月份,郑州市商品住宅销售均价达10243元/平方米,涨幅29.04%;其中郑东新区部分楼盘均价突破2.5万元/平方米。',\n",
       " '传英特尔欲分四批发行70亿美元债劵,年限最长的是规模20亿美元的30年期债劵,收益率可能比同期美国国债高出1个百分点',\n",
       " '舒城男子将妻子灌醉后带走家中现金,后与同事私奔;两人因被妻子算计,起诉离婚。',\n",
       " '瑞昌市政协党组成员、政协副主席徐新民涉嫌严重违纪,目前正接受组织调查。',\n",
       " '衡阳首例谎报案诈医院一院长,案发当晚医院中断了接诊',\n",
       " '汉中住建局官员面带微笑迎接业主,称该局领导微笑相迎业主是“笑脸相迎”。',\n",
       " '组图:武昌梁子湖现大量死亡乌龟,专家称系中外乌龟混战引起。',\n",
       " '唐山今日3时32分发生3.3级地震,与1976年唐山大地震时间仅相隔10分钟,地震中唐山主城区多处建筑倒塔砸到民房。',\n",
       " '常州市发布短时雨强橙色预警:预计今天夜里到2日,受冷暖空气交汇影响,我市将出现一次中等强度的降水过程。',\n",
       " '墨西哥头号大毒枭古斯曼越狱逃跑,看守最严的监狱被派遣大量警察搜查,机场航班被临时取消。',\n",
       " '福建省发布霜冻黄色预警:受冷空气影响,23日早晨,全省气温较低,南平市和宁德、三明、龙岩、泉州四市的西北部最低气温可达...',\n",
       " '中国重工自今日开市起停牌,因中船重工正推进风帆股份相关资产重组,具体包括风帆股份控股股东及相关资产进行出售。',\n",
       " '盐城一男子酒后去情人家里,被情人的老公砍死;因知道老婆外遇,愤怒男子砍断自己的左肩,左手死死拿住菜刀不放。',\n",
       " 'NASA宣布发现“最接近地球的行星”,网友纷纷表示:“移民”无戏称;NASA公布数据开普勒452b体积约有地球60%,位于天鹅座。',\n",
       " '川西和东北部有大雨 绵阳、德阳、成都3市部分地方有大雨,局部暴雨;阿坝州东部局部地方有大中雨。',\n",
       " '沪深股市市值超10万亿;今年以来沪指上涨近60%,深证涨幅120%',\n",
       " '美军无人机近日击毙也门最高头目纳赛尔·武海希,证实其已被击毙。',\n",
       " '呼市一酒吧起火引燃LED广告牌,窗户玻璃大量倾泻,无人员伤亡。',\n",
       " '王老吉诉加多宝“中国每卖10罐凉茶,7罐加多宝”广告构成虚假宣传不正当竞争案二审今日开审,法院未宣判,双方此前已因不正当竞争案提起数十起诉讼',\n",
       " '今晨,深圳一酒店爆炸后冒黑烟,核心区污染物指标略有上升;搜救部队已搜索被困百姓,搜寻因爆炸不顾危险回来收拾钱财群众。',\n",
       " '长沙一台的士车窗冒猪蹄味引发热议:湖南一出租车车主自曝车上的“猪蹄车”车牌系龙骧出租汽车公司',\n",
       " '湖北省国资委副主任鲁力军因涉嫌严重违纪被免职,曾任该集团党委书记、委员。',\n",
       " '中日两国在安全、政治、经济等评价项目上得分差距大,俄居榜首;去年被曝在克里米亚问题上与联合国发生冲突。',\n",
       " '教育部:将加快巡视频率,扩大巡视范围,全年拟派出20个巡视组进驻高校',\n",
       " '中国足协公布备战世预赛的45人名单,恒大11人领衔广东,鲁能国安各6人,上港5人',\n",
       " '组图:大凉山老师分享大山心声:妈妈病重时,女儿想念母亲泪水成河;爸爸四年前去世,女儿念叨“妈妈已死了”。',\n",
       " '郑州市三门峡市原副市长张英焕涉嫌受贿700万,一审获刑14年,其家属退还赃款68万余元。',\n",
       " '金价创雷曼倒闭以来最长连跌纪录,ETP持仓触及7年低点,多数基金经理持有黄金 ⁇ 多单。',\n",
       " '王军霞前夫黄天文新书首签售,曝光了很多与王军霞有关的独家内幕消息;自称当初写自传并非想离婚,本意挽救婚姻。',\n",
       " '知情消息人士称,印度AdaniGroup正与日本软银和台湾鸿海富士康进行商谈,争取他们对印度国内一规模30亿美元的太阳能电池和面板项目的投资。',\n",
       " '成都地铁五号线分两期建成,规划至2019年开通运营;一期工程全长50余公里,2016年4月开工建设',\n",
       " '兰州:男子与伙伴争执闹上街 当街持刀砍死同僚1死1伤',\n",
       " '中企首趟出海,南非启动国内投资800亿核电招标,国家核电首发将采用第三代核电技术,系国内功率最大功率最大的第三代核电技术。',\n",
       " '圆通回应今日早上快件起火:为仓库起火引发,客户先上网查询快件到站时间。先行赔付、向最初责任人进行追偿。',\n",
       " '呼和浩特疑犯疑妻子与同乡有染,持刀闯入同乡家连捅2人致其死亡。',\n",
       " '中国工程院院士:当前厄尔尼诺事件不会升级为“超级厄尔尼诺”',\n",
       " '金华新区管委会原主任陈陆一被开除党籍、行政开除,涉嫌犯罪问题移送司法机关。',\n",
       " '13个希特勒画布齐聚法兰克福,一纸13万欧元画“天鹅堡”成全球新地标;有网友表示“想给希特勒捐十万辆车。”',\n",
       " '四川:大邑发生直升飞机坠毁,两名伤员获救,事发时两名驾驶员已坠机,为俄制米171运输机',\n",
       " '扬州:大江化工污染事件涉多名官员,官方通报官员已有2名厅级、5名科级。',\n",
       " '今年来1306名外国籍海外人才获中国绿卡,占中国“绿卡”入选人数的1/3',\n",
       " '网曝《鬼吹灯》中“东夏国”遗址被找到,专家称其出土地位于辽宁铁岭;网友调侃称三叔或为外星人、吴邪',\n",
       " '美国前总统奥巴马在巴拿马与古巴领导人卡斯特罗见面,二人互致问候。',\n",
       " '日本称3中国海警船在钓鱼岛附近海域作业,并发出“禁止靠近日本领海”的警告。',\n",
       " '昨晚,一组三轮车与大货车相撞的大照片曝光,现造成7人死亡、1人受伤,伤亡者全部为三轮上人员;事故仍在处理中。',\n",
       " '外媒称哥伦比亚美女模特朱莉安娜·洛佩斯近日被逮捕,或面临死刑。',\n",
       " '军队网曝军队惊天黑幕 涉军谣言谣言被查',\n",
       " '[拌饭]黄磊二胎小女儿正面照首曝光,萌脸像多多;柯震东“夜店重聚”,嫩模作陪到天亮;32岁主持边策涉毒坠亡,好友悼念\"来世不要再走错\"。',\n",
       " '英国防部:英国皇家空军一架台风战斗机“起飞拦截两架俄罗斯军机”,称“俄军没有越境闯入英领空”,此前多次发生类似事件。',\n",
       " '“千颂伊”全智贤健身房被偷拍 上身做运动时被人偷拍',\n",
       " '黄奕承认与黄毅清离婚,称将以全部的爱陪女儿成长;声明中称黄毅清发微博“如释重负”。',\n",
       " '网传“黑龙江一高校明年拟新开56个专业”校方回应:据了解,今年学校仅申报新增投资学和机械工艺技术2门专业。',\n",
       " '石家庄市发布霾橙色预警:预计今天白天到夜间,石家庄持续有中度或重度霾,大气扩散能力弱,能见度差,污染严重,请防范!...',\n",
       " '中新网5月14日登封发生煤矿冒顶事故,致3人死亡3人失踪,已初步查明事故原因正在调查',\n",
       " '安庆一女生买樟脑丸,在宿舍投毒室友开水瓶,被警方发现后抓获;女生已交给校方处理',\n",
       " '今日晚,郑州一男子持匕首闯入孕妇楼持刀砍死孕妇,后持刀将孕妇砍成重伤;警方称该孕妇系嫌犯妻子。',\n",
       " '宁德宁古路东胡隧道昨天上午发生货车与小车交会碰撞事故,死者系2男1女与1婴幼儿。',\n",
       " '日本开发出新型电池,有望兼具传感器和电源的作用,在失能老人护理中可用尿液发电。',\n",
       " '攀枝花市发布高温橙色预警:受副热带高压和低槽共同影响,攀枝花市最高气温预计将达37度。',\n",
       " '郑州市民中得96万元一等奖,因无人认领未兑奖,如今这注6年前中过大奖的彩票变成弃奖。',\n",
       " '莆田市发布大风黄色预警:受冷空气影响,预计今天夜里我市沿海东北风7~8级阵风9级,请轮渡码头、出海船只、海上作业人员...',\n",
       " '反映河南省扶沟检察院检察长薄玉龙被判刑2年半,目前又回归检察院,并称已向领导行贿7万五千元',\n",
       " '警方称,“超速王”一年内60次超速违章,罚款1万余元。',\n",
       " '一名12岁女孩在法新社报道的喀麦隆北部城市发动自杀式炸弹袭击,2名学生和1人死亡',\n",
       " '福州:5旬老翁因嫖娼被警方抓到,才知道原来对方竟是男儿身;老人在网上被卖淫,被警察带走',\n",
       " '交通银行系统崩溃:取钱小心的ATM机被吞钱,银行客服繁忙;公交地铁多位用户报警,称ATM机无法出钞。',\n",
       " '女子制造伪造商品房买卖合同等材料后,从贷款公司贷出500万元偿还旧账。',\n",
       " '西洋水鬼因重伤本赛季报销,拜仁下周将战巴萨',\n",
       " '最高检局长透露中方正在向美国提供一份追捕贪官“优先名单”,请求美方协助追捕,名单上的人都藏匿在美国。',\n",
       " '汉中:汉中东汉峡90后男子醉酒和父亲发生争吵,随后将父亲掐死,伪造现场后向其子女隐瞒尸体被烧死。',\n",
       " '中国联合石油本月买48船原油,相当于去年10月买入数量的两倍,同时预期今年晚些时会有更多的可用储能用于战略储备。',\n",
       " '威廉·卡梅伦表示,苏格兰威士忌是英国统一的基础,其销售苏格兰威士忌每秒钟增加135英镑。',\n",
       " '金正恩已遭联合国安理会授权,其或将被作为反人类罪被送交国际刑事法庭;朝鲜日前称将加强核武备战。',\n",
       " '今上午8时许,广州华快三期永泰至嘉禾段大堵车,一辆面包车与一辆货车相撞,面包车上四人三死一伤。',\n",
       " '李小璐晒女儿甜美自拍照:晒娃晒到艺术,一袭花裙,文艺中带着萌点',\n",
       " '洛南1母猪生下“双头三眼”猪崽,2头胳膊上有眼睛;专家称该猪只存万分之一的概率。',\n",
       " '吴宗宪诈骗案一审被判1年10月徒刑;据了解,吴在半导体LED行业亏损3年,案发时为3千万元。',\n",
       " '男子担心女友知真相,冒用80后身份证件,乘飞机被机场安检人员拦下;被警方处予行政拘留5日',\n",
       " '日本拟今年夏天发射日本首个月球探测器,用人工智能“偷”得月亮资源,此前研究人员曾成功实现人类行走。',\n",
       " '龙南一妇女骑摩托骑进路边积水路面,不慎跌入水里溺亡;路面积水严重导致摩托车被淹没',\n",
       " '中国重工:中船重工拟通过向中船重工旗下船舶动力业务投资入股等方式,参与中船重工打造的动力业务平台等。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': 0.49659550301358446,\n",
       " 'rouge-2': 0.3007335561279025,\n",
       " 'rouge-l': 0.4081939467759942}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_chinese import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# 用Rouge评估之前要先将中文字符用空格隔开，所以BART和GPT-2是因为空格多了才导致BLEU和Rouge为0！\n",
    "docode_preds = [\" \".join(p) for p in result]\n",
    "decode_labels = [\" \".join(l) for l in ds[\"test\"][\"title\"]]\n",
    "scores = rouge.get_scores(docode_preds, decode_labels, avg=True)\n",
    "{\n",
    "    \"rouge-1\": scores[\"rouge-1\"][\"f\"],\n",
    "    \"rouge-2\": scores[\"rouge-2\"][\"f\"],\n",
    "    \"rouge-l\": scores[\"rouge-l\"][\"f\"],\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
