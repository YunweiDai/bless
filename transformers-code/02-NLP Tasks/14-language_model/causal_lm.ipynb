{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因果语言模型训练实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "# AutoModelForMaskedLM改成了AutoModelForCausalLM，因果语言模型可以用DataCollatorForLanguageModeling也可以用DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk(\"./wiki_cn_filtered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'completion'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'wikipedia.zh2307',\n",
       " 'completion': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆，馆长是锺明善。\\n历史\\n2004年9月20日开始筹建，2013年4月8日正式建成开馆，位于西安交通大学兴庆校区陕西省西安市咸宁西路28号。建筑面积6,800平米，展厅面积4,500平米，馆藏文物4,900余件。包括历代艺术文物馆、碑石书法馆、西部农民画馆、邢良坤陶瓷艺术馆、陕西秦腔博物馆和书画展厅共五馆一厅。\\n营业时间\\n* 周一至周六：上午九点至十二点，下午一点至五点\\n* 周日闭馆\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/PLM/bloom-1b4-zh\")\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"completion\"]]\n",
    "    return tokenizer(contents, max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 因果语言模型的预训练会自动生成labels，虽然也只是把input_ids重复了一遍！而且都要计算loss\n",
    "# 微调的时候可不是这样，原始labels以外的部分都要改成-100（[EOS]除外），包括input_ids和padding\n",
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3, 14722, 39341, 15213,\n",
       "            928, 37555,    10,   256,   577,   387,   479,   683,  5856,   435,\n",
       "          38191,   927, 27482,  5699, 14722,  7149, 23635, 15213,   355,  5742,\n",
       "           1532,   584, 31928,  1441,  4041,   672,  6603,   189,  7323,  4541,\n",
       "          10467,  3590,  7509,  1351,   355, 42089, 10865,  4882, 15131,  1390,\n",
       "           5742,   355,  5699, 14722, 39341,  4692,  7827, 29300, 33817, 14722,\n",
       "           1540, 13450,  6888, 26838,  2213,  2137,   420, 33315,    25,    15,\n",
       "           8334, 44211,   355,  1526,  6292,  6796,    23,    15,  4402, 44211,\n",
       "            355,  5742,  4784, 12950,    23,    15, 11834,  5206,  1764,   420,\n",
       "           2542, 35366,  8849, 12950,  5742,   554, 10937,  2840, 24306,  5742,\n",
       "            554, 10923, 15767,  4693,  5742,   554, 24475,  4781, 19441, 29463,\n",
       "           8849,  5742,   554, 19811,  7561, 12352, 15213,   642, 38057,  1526,\n",
       "           6292,  1740,  2134,  5742,   570,  6292,   672, 18341,  3577,   189,\n",
       "             13, 26390, 31754, 37633,  1038, 12697,  3495,  1871,  1546,  7141,\n",
       "           1871,   355, 10088,  9486,  1546,  2134,  1871,   189,    13, 26390,\n",
       "            821,  8545,  5742,     2],\n",
       "         [  587, 13203,   743,   355,  9240,  2385,   928, 19120,  5419,   581,\n",
       "          17573,  1471,  1011,   307,   387,   355,  2067,  9027,   782,  9240,\n",
       "           2226,  7062, 13601,  3906,  8402,  1545,  1570,  1030, 20287, 20423,\n",
       "           3389,   355,  6770,  4086,  2776, 23777, 18179, 15760,  4193,  1545,\n",
       "          13744,  9832,  3389,   420,  4433,  4572,  5753,  9240,  2226,  4909,\n",
       "          42360,   355,  1411,  9240,  2385, 37190,  3389,  9926, 21526, 18917,\n",
       "           6374, 15232, 34461, 27060,  2226, 18061,   658,  6391,  9240,   587,\n",
       "          18179,  1570,  9843,  6384, 15798,  6359, 15868,  4605, 33484,   355,\n",
       "           9662, 24608,  1489, 17992, 35318, 11481, 34020, 12094,  1143,  5139,\n",
       "          18179,  7721, 26136,  8439,   387,    72,   272,   840,   455,    72,\n",
       "            927,  9743, 23592,  2000,  1102,  5883,   420,  4672,  9240, 29901,\n",
       "          14265,   554, 12095, 39363,   355,  1293,  5904,   587, 16827, 37160,\n",
       "          13203,   916, 22677,  1570, 22205, 23352,  1380, 22137, 32856,   672,\n",
       "           9240, 29901, 19266,  6438, 25503, 13280, 14198, 14877,  1058, 10807,\n",
       "            355,  7895,   718,  1864,   782,  7066,   355, 12846, 11293,  1321,\n",
       "           1715, 18072,  7720,  1667,  9240,  2385,   420, 22725, 29187, 12095,\n",
       "          19823, 30196,  2481,  9240,  2385,   355,  7344,  2953, 15217,  9039,\n",
       "            373, 34317,  5904,   420,  9240,  2385,   587,  9364, 22003,  1326,\n",
       "          13601, 35127, 11946,   978,  4562, 12095,   420, 13280, 18179,   916,\n",
       "          10807,   355, 25593, 10661,   355,  9240,  2385, 15243,  2124,  5314,\n",
       "           5139, 34977,  7916,   420, 12695,  9240, 39719, 13325, 17722,  9240,\n",
       "           2385,  7938, 12095,   355,  1032,  4401,  5375, 15788, 29845, 30544,\n",
       "            672, 17038,   189,  1185, 17038, 13280,  6692,  1143,  3383,  3861,\n",
       "            623,  9656,  1139,   903, 15476,   672, 11308,   189,    13,   210,\n",
       "           7556,  3906, 10536,  6569,  9240,  2385,   928,  4628,   718,  3853,\n",
       "           1864, 11743,   927,   189,    13,   210, 18236, 13203,  9240,  2385,\n",
       "            189,    13, 27425,  3906,  9240,  2385,   928,  5139,  3861,  9910,\n",
       "          31519,  9240,  2385,  1328, 12516,  2133, 32346, 31519, 13203,  9240,\n",
       "           2385, 14858,  5139,  3861, 18048,   927,   189,    13, 31123, 13203,\n",
       "           9240,  2385,   189,    13, 31123,  4182,  3906,  9240,  2385,   928,\n",
       "           4628,  2542,  3413,  1261, 21683,   554,  3674,  3252,  1781,   642,\n",
       "           3220,  2187,  8573,   927,   189,    13,   210, 17404,  9240,  2385,\n",
       "            189,    13, 11135,   108, 14225, 16495, 33413,  5011,  7105,   743,\n",
       "            928, 18255, 12217, 24325,  9985,  1228,  9833,  1285, 13203,  5011,\n",
       "           7105,   743, 23214,   189,    13,   210, 22223,  1559, 13203,  9240,\n",
       "           2385,   189,    13,   210, 27257, 32060,  6805,   114,   240,   170,\n",
       "            100,   124, 45913,   228,  6805,   100,   124, 45913,   228,   171,\n",
       "            238,   224, 28780, 27538, 12334, 12217, 24325,  9985,  1228,  9833,\n",
       "           1285, 12334, 13203,  5011]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100, 14722, 39341, 15213,\n",
       "            928, 37555,    10,   256,   577,   387,   479,   683,  5856,   435,\n",
       "          38191,   927, 27482,  5699, 14722,  7149, 23635, 15213,   355,  5742,\n",
       "           1532,   584, 31928,  1441,  4041,   672,  6603,   189,  7323,  4541,\n",
       "          10467,  3590,  7509,  1351,   355, 42089, 10865,  4882, 15131,  1390,\n",
       "           5742,   355,  5699, 14722, 39341,  4692,  7827, 29300, 33817, 14722,\n",
       "           1540, 13450,  6888, 26838,  2213,  2137,   420, 33315,    25,    15,\n",
       "           8334, 44211,   355,  1526,  6292,  6796,    23,    15,  4402, 44211,\n",
       "            355,  5742,  4784, 12950,    23,    15, 11834,  5206,  1764,   420,\n",
       "           2542, 35366,  8849, 12950,  5742,   554, 10937,  2840, 24306,  5742,\n",
       "            554, 10923, 15767,  4693,  5742,   554, 24475,  4781, 19441, 29463,\n",
       "           8849,  5742,   554, 19811,  7561, 12352, 15213,   642, 38057,  1526,\n",
       "           6292,  1740,  2134,  5742,   570,  6292,   672, 18341,  3577,   189,\n",
       "             13, 26390, 31754, 37633,  1038, 12697,  3495,  1871,  1546,  7141,\n",
       "           1871,   355, 10088,  9486,  1546,  2134,  1871,   189,    13, 26390,\n",
       "            821,  8545,  5742,     2],\n",
       "         [  587, 13203,   743,   355,  9240,  2385,   928, 19120,  5419,   581,\n",
       "          17573,  1471,  1011,   307,   387,   355,  2067,  9027,   782,  9240,\n",
       "           2226,  7062, 13601,  3906,  8402,  1545,  1570,  1030, 20287, 20423,\n",
       "           3389,   355,  6770,  4086,  2776, 23777, 18179, 15760,  4193,  1545,\n",
       "          13744,  9832,  3389,   420,  4433,  4572,  5753,  9240,  2226,  4909,\n",
       "          42360,   355,  1411,  9240,  2385, 37190,  3389,  9926, 21526, 18917,\n",
       "           6374, 15232, 34461, 27060,  2226, 18061,   658,  6391,  9240,   587,\n",
       "          18179,  1570,  9843,  6384, 15798,  6359, 15868,  4605, 33484,   355,\n",
       "           9662, 24608,  1489, 17992, 35318, 11481, 34020, 12094,  1143,  5139,\n",
       "          18179,  7721, 26136,  8439,   387,    72,   272,   840,   455,    72,\n",
       "            927,  9743, 23592,  2000,  1102,  5883,   420,  4672,  9240, 29901,\n",
       "          14265,   554, 12095, 39363,   355,  1293,  5904,   587, 16827, 37160,\n",
       "          13203,   916, 22677,  1570, 22205, 23352,  1380, 22137, 32856,   672,\n",
       "           9240, 29901, 19266,  6438, 25503, 13280, 14198, 14877,  1058, 10807,\n",
       "            355,  7895,   718,  1864,   782,  7066,   355, 12846, 11293,  1321,\n",
       "           1715, 18072,  7720,  1667,  9240,  2385,   420, 22725, 29187, 12095,\n",
       "          19823, 30196,  2481,  9240,  2385,   355,  7344,  2953, 15217,  9039,\n",
       "            373, 34317,  5904,   420,  9240,  2385,   587,  9364, 22003,  1326,\n",
       "          13601, 35127, 11946,   978,  4562, 12095,   420, 13280, 18179,   916,\n",
       "          10807,   355, 25593, 10661,   355,  9240,  2385, 15243,  2124,  5314,\n",
       "           5139, 34977,  7916,   420, 12695,  9240, 39719, 13325, 17722,  9240,\n",
       "           2385,  7938, 12095,   355,  1032,  4401,  5375, 15788, 29845, 30544,\n",
       "            672, 17038,   189,  1185, 17038, 13280,  6692,  1143,  3383,  3861,\n",
       "            623,  9656,  1139,   903, 15476,   672, 11308,   189,    13,   210,\n",
       "           7556,  3906, 10536,  6569,  9240,  2385,   928,  4628,   718,  3853,\n",
       "           1864, 11743,   927,   189,    13,   210, 18236, 13203,  9240,  2385,\n",
       "            189,    13, 27425,  3906,  9240,  2385,   928,  5139,  3861,  9910,\n",
       "          31519,  9240,  2385,  1328, 12516,  2133, 32346, 31519, 13203,  9240,\n",
       "           2385, 14858,  5139,  3861, 18048,   927,   189,    13, 31123, 13203,\n",
       "           9240,  2385,   189,    13, 31123,  4182,  3906,  9240,  2385,   928,\n",
       "           4628,  2542,  3413,  1261, 21683,   554,  3674,  3252,  1781,   642,\n",
       "           3220,  2187,  8573,   927,   189,    13,   210, 17404,  9240,  2385,\n",
       "            189,    13, 11135,   108, 14225, 16495, 33413,  5011,  7105,   743,\n",
       "            928, 18255, 12217, 24325,  9985,  1228,  9833,  1285, 13203,  5011,\n",
       "           7105,   743, 23214,   189,    13,   210, 22223,  1559, 13203,  9240,\n",
       "           2385,   189,    13,   210, 27257, 32060,  6805,   114,   240,   170,\n",
       "            100,   124, 45913,   228,  6805,   100,   124, 45913,   228,   171,\n",
       "            238,   224, 28780, 27538, 12334, 12217, 24325,  9985,  1228,  9833,\n",
       "           1285, 12334, 13203,  5011]])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(dl)) # 这个batch中第一条被left padding，eos token存在；第二条被right truncation，eos token不存在。这个似乎并没有什么关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(46145, 2048)\n",
       "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=46145, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/data/PLM/bloom-1b4-zh\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./causal_lm\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_ds,\n",
    "    # 上面那个DataCollatorForLanguageModeling只是取样查看，真正的DataCollatorForLanguageModeling要用在这！\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 14:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.091700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=312, training_loss=3.214142547203944, metrics={'train_runtime': 853.1351, 'train_samples_per_second': 11.721, 'train_steps_per_second': 0.366, 'total_flos': 2.685584078733312e+16, 'train_loss': 3.214142547203944, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安市经开区的综合性博物馆，位于西安交通大学校区内的科技大楼内。博物馆于2020年9月8日随交通大学百年校庆正式开放，馆内共分为9个常设展厅、6个临时展厅。该博物馆隶属于西安交通大学博物馆研究部。\\n院史馆\\n展厅号: W1\\n简介:\\n院史馆位于西安交通大学科技大楼二楼，共2层，总展览面积约300平方米。该馆馆名由西安交通大学历史悠久的百年校训“治学严谨、为人敦厚”\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常繁荣，游戏产业的产值越来越大，也有游戏厂商为吸引用户，不断推出各种低门槛的游戏，其中《街头足球》和《足球经理》表现优异，但是《FIFA 18》和《FIFA 19》尚未公布，这两款游戏是针对《FIFA 18》和《FIFA 19》游戏发售后的续作而设立的。其中《街头足球》在发售首周就吸引了超过700万用户的关注，虽然《FIFA 18》和《FIFA 19》尚未发行，但是凭借着这两个游戏，已经让游戏市场火了大半年。\\n下面是一则游戏游戏新闻。\\n'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常\", max_length=128, do_sample=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
