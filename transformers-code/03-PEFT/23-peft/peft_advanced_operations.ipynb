{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT 进阶操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 自定义模型适配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from peft import LoraConfig, get_peft_model, PeftModel \n",
    "# LoraConfig设置Lora的相关参数；get_peft_model向模型中加入未经训练的Lora；PeftModel用于加载训练好的Lora模型然后与原模型合并（model.merge_and_unload()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.float32\n",
      "0.bias torch.float32\n",
      "2.weight torch.float32\n",
      "2.bias torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in net1.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules={'0'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LoraConfig(target_modules=[\"0\"]) # 第0层的名称就是\"0\"，task_type因为是自定义的模型所以不存在\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_peft_model(net1, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=10, out_features=10, bias=True\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 多适配器加载与切换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以对每个任务都使用不同的Lora，然后根据具体任务进行加载与切换，太妙了！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoraConfig(target_modules=[\"0\"])\n",
    "model2 = get_peft_model(net2, config1)\n",
    "model2.save_pretrained(\"./loraA\") # loraA和loraB不是一对Lora矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = LoraConfig(target_modules=[\"2\"])\n",
    "model2 = get_peft_model(net2, config2)\n",
    "model2.save_pretrained(\"./loraB\") # 而是两个不同的Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(\n",
       "    in_features=10, out_features=10, bias=True\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Identity()\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(\n",
       "    in_features=10, out_features=2, bias=True\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Identity()\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=2, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 # get_peft_model同样会影响net2，只不过net2没有PeftModel、LoraModel的包覆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=10, out_features=10, bias=True\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(\n",
       "        in_features=10, out_features=2, bias=True\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 # 注意与net2之间的区别：PeftModel、LoraModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=10, out_features=10, bias=True\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (loraA): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (loraA): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (loraA): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = PeftModel.from_pretrained(net2, model_id=\"./loraA/\", adapter_name=\"loraA\") # adapter_name对适配器进行命名，不指定会默认为default！\n",
    "# 注意将这个loraA与实际的lora_A相区分！\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(\n",
       "        in_features=10, out_features=10, bias=True\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (loraA): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (loraA): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (loraA): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(\n",
       "        in_features=10, out_features=2, bias=True\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (loraB): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (loraB): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (loraB): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_adapter(\"./loraB/\", adapter_name=\"loraB\") # 此时model2已经是PeftModel类型了，可以直接load_adapter而不用from_pretrained\n",
    "model2 # 注意必须指定adapter_name，因为之前已经有adapter被合并了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loraA'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.active_adapter # 活跃的adapter默认是loraA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2133, -0.2173]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0, 10).view(1, 10).float()) # 注意此时loraA的lora_B为0，所以Lora其实并没有什么卵用\n",
    "# 默认处于eval状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.0.weight Parameter containing:\n",
      "tensor([[ 0.1104, -0.1041, -0.0983, -0.1713, -0.1536,  0.2697, -0.1695,  0.2809,\n",
      "          0.2345,  0.1055],\n",
      "        [ 0.0013,  0.0923,  0.0483, -0.3077,  0.2787, -0.0782,  0.2527,  0.2126,\n",
      "          0.2008, -0.3018],\n",
      "        [ 0.0544, -0.2970, -0.0767, -0.0035, -0.2246,  0.2156, -0.0449,  0.1619,\n",
      "         -0.1122, -0.2874],\n",
      "        [-0.1588,  0.2359,  0.2355,  0.2573,  0.0462,  0.2305, -0.1351, -0.1660,\n",
      "          0.2975, -0.1043],\n",
      "        [-0.2665, -0.2501,  0.1441,  0.0415,  0.1242, -0.1810, -0.0929, -0.2947,\n",
      "         -0.1736, -0.1115],\n",
      "        [-0.2274,  0.0383,  0.0497,  0.2918, -0.1197, -0.0032,  0.0781, -0.1618,\n",
      "         -0.2034,  0.1531],\n",
      "        [ 0.0616, -0.0254, -0.1494, -0.1608, -0.1331, -0.0720,  0.0710, -0.2306,\n",
      "          0.0889, -0.1325],\n",
      "        [-0.1672,  0.0955,  0.1939, -0.1365, -0.1152,  0.0280, -0.2409,  0.2064,\n",
      "          0.0816, -0.2088],\n",
      "        [-0.2774,  0.2275, -0.0322,  0.2042,  0.0296,  0.2570, -0.0759,  0.2561,\n",
      "          0.2366,  0.1029],\n",
      "        [ 0.1700, -0.1721,  0.0088, -0.0043, -0.2538,  0.1994, -0.3054, -0.0200,\n",
      "         -0.1674,  0.2076]])\n",
      "base_model.model.0.bias Parameter containing:\n",
      "tensor([ 0.0586, -0.1124,  0.0161,  0.2842,  0.1645,  0.1215,  0.0239,  0.2677,\n",
      "         0.1704,  0.0589])\n",
      "base_model.model.0.lora_A.loraA.weight Parameter containing:\n",
      "tensor([[-0.0879,  0.2033, -0.2134,  0.0032,  0.1831, -0.1490, -0.0849,  0.0960,\n",
      "         -0.2649, -0.2814],\n",
      "        [ 0.0963, -0.1174,  0.1352, -0.2002,  0.2779, -0.1031,  0.0647,  0.1022,\n",
      "         -0.2593, -0.0805],\n",
      "        [ 0.2299,  0.2916,  0.2013,  0.1455,  0.0478, -0.0565, -0.0162,  0.2051,\n",
      "          0.0158, -0.1017],\n",
      "        [ 0.1763, -0.0267, -0.2387, -0.1565,  0.0074,  0.3006,  0.3047, -0.1279,\n",
      "          0.1176,  0.0037],\n",
      "        [ 0.1714,  0.1707,  0.2002,  0.1908,  0.2003, -0.0610, -0.1955,  0.1863,\n",
      "         -0.2751, -0.0572],\n",
      "        [-0.2792,  0.2075, -0.2822, -0.1672, -0.2855,  0.3029, -0.0133,  0.0787,\n",
      "          0.2063,  0.0908],\n",
      "        [-0.0943,  0.1981, -0.0026, -0.1637, -0.0691,  0.0042, -0.1646, -0.1445,\n",
      "          0.1072, -0.1196],\n",
      "        [ 0.2457, -0.2890, -0.0975,  0.2241,  0.0356,  0.2681,  0.0967, -0.2401,\n",
      "          0.0876,  0.1161]])\n",
      "base_model.model.0.lora_B.loraA.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "base_model.model.2.weight Parameter containing:\n",
      "tensor([[ 0.1042,  0.1579,  0.0146,  0.1640,  0.1315,  0.1033, -0.0313, -0.3019,\n",
      "         -0.2383,  0.0561],\n",
      "        [ 0.3059, -0.2575,  0.0538, -0.1582,  0.0072, -0.0157,  0.1009, -0.2412,\n",
      "         -0.0403,  0.2798]])\n",
      "base_model.model.2.bias Parameter containing:\n",
      "tensor([ 0.2453, -0.2403])\n",
      "base_model.model.2.lora_A.loraB.weight Parameter containing:\n",
      "tensor([[ 0.1396, -0.0717,  0.2805,  0.0185,  0.1406, -0.2566,  0.1039, -0.0946,\n",
      "         -0.1337,  0.2985],\n",
      "        [ 0.0595, -0.0846, -0.0318, -0.0643, -0.2652,  0.2636,  0.2406,  0.2299,\n",
      "         -0.0037, -0.2908],\n",
      "        [ 0.1143,  0.2116, -0.2006, -0.0642, -0.1659, -0.2628,  0.0107,  0.1523,\n",
      "          0.1805,  0.1434],\n",
      "        [-0.2216, -0.2243,  0.1462, -0.1144,  0.2183,  0.0604,  0.1321,  0.1398,\n",
      "         -0.1128, -0.0940],\n",
      "        [-0.0883, -0.1899, -0.0528,  0.0900,  0.0453, -0.0466, -0.2527,  0.1065,\n",
      "          0.2849, -0.2634],\n",
      "        [-0.0815,  0.1869, -0.3158,  0.0562,  0.0469, -0.1481,  0.1545, -0.2746,\n",
      "         -0.2750, -0.2047],\n",
      "        [ 0.2872, -0.2812, -0.3049, -0.1787, -0.2580, -0.1931,  0.2154,  0.1171,\n",
      "          0.2017, -0.2483],\n",
      "        [-0.0858, -0.1847,  0.1592, -0.2475, -0.0530,  0.0799, -0.2422, -0.2690,\n",
      "         -0.0900,  0.1600]])\n",
      "base_model.model.2.lora_B.loraB.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    if name in [\"base_model.model.0.lora_A.loraA.weight\", \"base_model.model.0.lora_B.loraA.weight\"]:\n",
    "        param.data = torch.ones_like(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57.0814, 11.6204]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0, 10).view(1, 10).float()) # 现在loraA才算有点用处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.set_adapter(\"loraB\") # 将活跃的adapter设置为loraB，变相禁用了loraA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loraB'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2133, -0.2173]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0, 10).view(1, 10).float()) # 注意此时loraB的lora_B也为0，所以Lora其实也并没有什么卵用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 禁用适配器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.set_adapter(\"loraA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57.0814, 11.6204]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2133, -0.2173]])\n"
     ]
    }
   ],
   "source": [
    "with model2.disable_adapter(): # Lora等adapter其实是在这里禁用的！之前都必须有某个adapter活跃\n",
    "    print(model2(torch.arange(0, 10).view(1, 10).float()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
